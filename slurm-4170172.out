Running "module reset". Resetting modules to system default. The following $MODULEPATH directories have been removed: None
WARNING:__main__:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/u/choprahetarth/all_files/starcoder2
/u/choprahetarth/all_files/starcoder2/u/choprahetarth/all_files/starcoder2

/u/choprahetarth/all_files/starcoder2
Loading BNB
Loading Model
Loading BNB
Loading BNB
Loading Model
Loading Model
Loading BNB
Loading Model

Downloading shards:   0%|          | 0/14 [00:00<?, ?it/s]
Downloading shards:   0%|          | 0/14 [00:00<?, ?it/s]
Downloading shards:   0%|          | 0/14 [00:00<?, ?it/s]
Downloading shards:   0%|          | 0/14 [00:00<?, ?it/s]
Downloading shards:   7%|▋         | 1/14 [02:04<26:58, 124.51s/it]
Downloading shards:   7%|▋         | 1/14 [02:04<26:58, 124.51s/it]
Downloading shards:   7%|▋         | 1/14 [02:04<26:58, 124.46s/it]
Downloading shards:   7%|▋         | 1/14 [02:04<26:59, 124.57s/it]
Downloading shards:  14%|█▍        | 2/14 [02:25<12:44, 63.74s/it] 
Downloading shards:  14%|█▍        | 2/14 [02:25<12:44, 63.72s/it] 
Downloading shards:  14%|█▍        | 2/14 [02:25<12:44, 63.74s/it] 
Downloading shards:  14%|█▍        | 2/14 [02:25<12:45, 63.82s/it] 
Downloading shards:  21%|██▏       | 3/14 [02:48<08:15, 45.08s/it]
Downloading shards:  21%|██▏       | 3/14 [02:48<08:15, 45.04s/it]
Downloading shards:  21%|██▏       | 3/14 [02:48<08:15, 45.08s/it]
Downloading shards:  21%|██▏       | 3/14 [02:48<08:16, 45.11s/it]
Downloading shards:  29%|██▊       | 4/14 [03:09<05:54, 35.41s/it]
Downloading shards:  29%|██▊       | 4/14 [03:09<05:53, 35.38s/it]
Downloading shards:  29%|██▊       | 4/14 [03:09<05:54, 35.41s/it]
Downloading shards:  29%|██▊       | 4/14 [03:09<05:54, 35.42s/it]
Downloading shards:  36%|███▌      | 5/14 [03:27<04:21, 29.08s/it]
Downloading shards:  36%|███▌      | 5/14 [03:27<04:21, 29.10s/it]
Downloading shards:  36%|███▌      | 5/14 [03:27<04:21, 29.08s/it]
Downloading shards:  36%|███▌      | 5/14 [03:27<04:21, 29.09s/it]
Downloading shards:  43%|████▎     | 6/14 [03:49<03:34, 26.84s/it]
Downloading shards:  43%|████▎     | 6/14 [03:49<03:34, 26.85s/it]
Downloading shards:  43%|████▎     | 6/14 [03:49<03:34, 26.85s/it]
Downloading shards:  43%|████▎     | 6/14 [03:49<03:35, 26.90s/it]
Downloading shards:  50%|█████     | 7/14 [04:12<02:58, 25.43s/it]
Downloading shards:  50%|█████     | 7/14 [04:12<02:58, 25.44s/it]
Downloading shards:  50%|█████     | 7/14 [04:12<02:58, 25.45s/it]
Downloading shards:  50%|█████     | 7/14 [04:12<02:58, 25.48s/it]
Downloading shards:  57%|█████▋    | 8/14 [05:46<04:44, 47.41s/it]
Downloading shards:  57%|█████▋    | 8/14 [05:46<04:44, 47.40s/it]
Downloading shards:  57%|█████▋    | 8/14 [05:46<04:44, 47.40s/it]
Downloading shards:  57%|█████▋    | 8/14 [05:46<04:44, 47.40s/it]
Downloading shards:  64%|██████▍   | 9/14 [06:39<04:05, 49.06s/it]
Downloading shards:  64%|██████▍   | 9/14 [06:39<04:05, 49.06s/it]
Downloading shards:  64%|██████▍   | 9/14 [06:39<04:05, 49.07s/it]
Downloading shards:  64%|██████▍   | 9/14 [06:39<04:05, 49.08s/it]
Downloading shards:  71%|███████▏  | 10/14 [07:04<02:46, 41.63s/it]
Downloading shards:  71%|███████▏  | 10/14 [07:04<02:46, 41.63s/it]
Downloading shards:  71%|███████▏  | 10/14 [07:04<02:46, 41.63s/it]
Downloading shards:  71%|███████▏  | 10/14 [07:04<02:46, 41.65s/it]
Downloading shards:  79%|███████▊  | 11/14 [07:21<01:42, 34.31s/it]
Downloading shards:  79%|███████▊  | 11/14 [07:21<01:42, 34.30s/it]
Downloading shards:  79%|███████▊  | 11/14 [07:22<01:43, 34.33s/it]
Downloading shards:  79%|███████▊  | 11/14 [07:21<01:42, 34.33s/it]
Downloading shards:  86%|████████▌ | 12/14 [07:43<01:00, 30.40s/it]
Downloading shards:  86%|████████▌ | 12/14 [07:43<01:00, 30.38s/it]
Downloading shards:  86%|████████▌ | 12/14 [07:43<01:00, 30.42s/it]
Downloading shards:  86%|████████▌ | 12/14 [07:43<01:00, 30.42s/it]
Downloading shards:  93%|█████████▎| 13/14 [08:05<00:27, 27.99s/it]
Downloading shards:  93%|█████████▎| 13/14 [08:05<00:27, 27.99s/it]
Downloading shards:  93%|█████████▎| 13/14 [08:05<00:28, 28.02s/it]
Downloading shards:  93%|█████████▎| 13/14 [08:06<00:28, 28.05s/it]
Downloading shards: 100%|██████████| 14/14 [08:27<00:00, 26.11s/it]
Downloading shards: 100%|██████████| 14/14 [08:27<00:00, 36.26s/it]

Downloading shards: 100%|██████████| 14/14 [08:27<00:00, 26.12s/it]
Downloading shards: 100%|██████████| 14/14 [08:27<00:00, 36.26s/it]

Downloading shards: 100%|██████████| 14/14 [08:27<00:00, 26.12s/it]
Downloading shards: 100%|██████████| 14/14 [08:27<00:00, 36.27s/it]

Downloading shards: 100%|██████████| 14/14 [08:27<00:00, 26.13s/it]
Downloading shards: 100%|██████████| 14/14 [08:27<00:00, 36.27s/it]

Loading checkpoint shards:   0%|          | 0/14 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/14 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/14 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/14 [00:00<?, ?it/s]
Loading checkpoint shards:   7%|▋         | 1/14 [00:03<00:40,  3.11s/it]
Loading checkpoint shards:   7%|▋         | 1/14 [00:03<00:45,  3.49s/it]
Loading checkpoint shards:   7%|▋         | 1/14 [00:03<00:45,  3.48s/it]
Loading checkpoint shards:   7%|▋         | 1/14 [00:03<00:45,  3.48s/it]
Loading checkpoint shards:  14%|█▍        | 2/14 [00:05<00:34,  2.90s/it]
Loading checkpoint shards:  14%|█▍        | 2/14 [00:06<00:39,  3.31s/it]
Loading checkpoint shards:  14%|█▍        | 2/14 [00:06<00:39,  3.32s/it]
Loading checkpoint shards:  14%|█▍        | 2/14 [00:06<00:39,  3.32s/it]
Loading checkpoint shards:  21%|██▏       | 3/14 [00:08<00:29,  2.67s/it]
Loading checkpoint shards:  21%|██▏       | 3/14 [00:09<00:34,  3.11s/it]
Loading checkpoint shards:  21%|██▏       | 3/14 [00:09<00:34,  3.11s/it]
Loading checkpoint shards:  21%|██▏       | 3/14 [00:09<00:34,  3.11s/it]
Loading checkpoint shards:  29%|██▊       | 4/14 [00:10<00:24,  2.40s/it]
Loading checkpoint shards:  36%|███▌      | 5/14 [00:11<00:19,  2.14s/it]
Loading checkpoint shards:  29%|██▊       | 4/14 [00:12<00:28,  2.89s/it]
Loading checkpoint shards:  29%|██▊       | 4/14 [00:12<00:28,  2.89s/it]
Loading checkpoint shards:  29%|██▊       | 4/14 [00:12<00:28,  2.90s/it]
Loading checkpoint shards:  43%|████▎     | 6/14 [00:13<00:15,  1.99s/it]
Loading checkpoint shards:  36%|███▌      | 5/14 [00:14<00:24,  2.67s/it]
Loading checkpoint shards:  36%|███▌      | 5/14 [00:14<00:24,  2.67s/it]
Loading checkpoint shards:  36%|███▌      | 5/14 [00:14<00:24,  2.68s/it]
Loading checkpoint shards:  50%|█████     | 7/14 [00:15<00:13,  1.90s/it]
Loading checkpoint shards:  43%|████▎     | 6/14 [00:16<00:20,  2.54s/it]
Loading checkpoint shards:  43%|████▎     | 6/14 [00:16<00:20,  2.54s/it]
Loading checkpoint shards:  43%|████▎     | 6/14 [00:16<00:20,  2.55s/it]
Loading checkpoint shards:  57%|█████▋    | 8/14 [00:17<00:11,  1.84s/it]
Loading checkpoint shards:  64%|██████▍   | 9/14 [00:18<00:08,  1.80s/it]
Loading checkpoint shards:  50%|█████     | 7/14 [00:19<00:17,  2.54s/it]
Loading checkpoint shards:  50%|█████     | 7/14 [00:19<00:17,  2.54s/it]
Loading checkpoint shards:  50%|█████     | 7/14 [00:19<00:17,  2.54s/it]
Loading checkpoint shards:  71%|███████▏  | 10/14 [00:20<00:07,  1.77s/it]
Loading checkpoint shards:  57%|█████▋    | 8/14 [00:21<00:14,  2.47s/it]
Loading checkpoint shards:  57%|█████▋    | 8/14 [00:21<00:14,  2.48s/it]
Loading checkpoint shards:  57%|█████▋    | 8/14 [00:21<00:14,  2.48s/it]
Loading checkpoint shards:  79%|███████▊  | 11/14 [00:22<00:05,  1.76s/it]
Loading checkpoint shards:  64%|██████▍   | 9/14 [00:23<00:11,  2.39s/it]
Loading checkpoint shards:  64%|██████▍   | 9/14 [00:23<00:11,  2.39s/it]
Loading checkpoint shards:  64%|██████▍   | 9/14 [00:23<00:11,  2.40s/it]
Loading checkpoint shards:  86%|████████▌ | 12/14 [00:23<00:03,  1.76s/it]
Loading checkpoint shards:  93%|█████████▎| 13/14 [00:25<00:01,  1.75s/it]
Loading checkpoint shards:  71%|███████▏  | 10/14 [00:25<00:09,  2.34s/it]
Loading checkpoint shards:  71%|███████▏  | 10/14 [00:25<00:09,  2.34s/it]
Loading checkpoint shards:  71%|███████▏  | 10/14 [00:25<00:09,  2.34s/it]
Loading checkpoint shards: 100%|██████████| 14/14 [00:27<00:00,  1.65s/it]
Loading checkpoint shards: 100%|██████████| 14/14 [00:27<00:00,  1.94s/it]
Model Loaded
Setting num_proc from 64 back to 1 for the train split to disable multiprocessing as it only contains one shard.

Generating train split: 0 examples [00:00, ? examples/s]
Loading checkpoint shards:  79%|███████▊  | 11/14 [00:28<00:06,  2.32s/it]
Loading checkpoint shards:  79%|███████▊  | 11/14 [00:28<00:06,  2.32s/it]
Loading checkpoint shards:  79%|███████▊  | 11/14 [00:28<00:06,  2.32s/it]
Generating train split: 15246 examples [00:00, 57774.28 examples/s]
Generating train split: 15246 examples [00:00, 57548.21 examples/s]

Map:   0%|          | 0/15246 [00:00<?, ? examples/s]
Map:  15%|█▍        | 2246/15246 [00:00<00:00, 22321.70 examples/s]
Map:  30%|███       | 4626/15246 [00:00<00:00, 23185.62 examples/s]
Map:  46%|████▌     | 6980/15246 [00:00<00:00, 23344.67 examples/s]
Map:  61%|██████▏   | 9342/15246 [00:00<00:00, 23446.81 examples/s]
Map:  77%|███████▋  | 11707/15246 [00:00<00:00, 23515.69 examples/s]
Map: 100%|█████████▉| 15185/15246 [00:00<00:00, 23369.83 examples/s]
Map: 100%|██████████| 15246/15246 [00:00<00:00, 23113.70 examples/s]
Sample from the training dataset:  {'prompt': 'Given this Ansible Name Field, please generate the ansible task. name: Ensure fail2ban service is enabled and started', 'completion': 'service:\n  name: fail2ban\n  state: started\n  enabled: true\nwhen: centos_base_fail2ban_configuration|bool\n'}
/u/choprahetarth/.conda/envs/scoder_2/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.

Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
  warnings.warn(message, FutureWarning)
/u/choprahetarth/.conda/envs/scoder_2/lib/python3.10/site-packages/transformers/training_args.py:1965: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.
  warnings.warn(
/u/choprahetarth/.conda/envs/scoder_2/lib/python3.10/site-packages/transformers/training_args.py:1965: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.
  warnings.warn(

Loading checkpoint shards:  86%|████████▌ | 12/14 [00:30<00:04,  2.31s/it]
Loading checkpoint shards:  86%|████████▌ | 12/14 [00:30<00:04,  2.31s/it]
Loading checkpoint shards:  86%|████████▌ | 12/14 [00:30<00:04,  2.32s/it]/u/choprahetarth/.conda/envs/scoder_2/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
  warnings.warn(

Loading checkpoint shards:  93%|█████████▎| 13/14 [00:32<00:02,  2.27s/it]
Loading checkpoint shards:  93%|█████████▎| 13/14 [00:32<00:02,  2.27s/it]
Loading checkpoint shards:  93%|█████████▎| 13/14 [00:32<00:02,  2.27s/it]
Loading checkpoint shards: 100%|██████████| 14/14 [00:34<00:00,  2.10s/it]
Loading checkpoint shards: 100%|██████████| 14/14 [00:34<00:00,  2.46s/it]

Loading checkpoint shards: 100%|██████████| 14/14 [00:34<00:00,  2.10s/it]
Loading checkpoint shards: 100%|██████████| 14/14 [00:34<00:00,  2.46s/it]

Loading checkpoint shards: 100%|██████████| 14/14 [00:34<00:00,  2.10s/it]
Loading checkpoint shards: 100%|██████████| 14/14 [00:34<00:00,  2.46s/it]
Model Loaded
Model Loaded
Model Loaded
Sample from the training dataset:  {'prompt': 'Given this Ansible Name Field, please generate the ansible task. name: Ensure fail2ban service is enabled and started', 'completion': 'service:\n  name: fail2ban\n  state: started\n  enabled: true\nwhen: centos_base_fail2ban_configuration|bool\n'}
Sample from the training dataset:  {'prompt': 'Given this Ansible Name Field, please generate the ansible task. name: Ensure fail2ban service is enabled and started', 'completion': 'service:\n  name: fail2ban\n  state: started\n  enabled: true\nwhen: centos_base_fail2ban_configuration|bool\n'}
Sample from the training dataset:  {'prompt': 'Given this Ansible Name Field, please generate the ansible task. name: Ensure fail2ban service is enabled and started', 'completion': 'service:\n  name: fail2ban\n  state: started\n  enabled: true\nwhen: centos_base_fail2ban_configuration|bool\n'}
/u/choprahetarth/.conda/envs/scoder_2/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.

Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
  warnings.warn(message, FutureWarning)
/u/choprahetarth/.conda/envs/scoder_2/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.

Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
  warnings.warn(message, FutureWarning)
/u/choprahetarth/.conda/envs/scoder_2/lib/python3.10/site-packages/transformers/training_args.py:1965: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.
  warnings.warn(
/u/choprahetarth/.conda/envs/scoder_2/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.

Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
  warnings.warn(message, FutureWarning)
/u/choprahetarth/.conda/envs/scoder_2/lib/python3.10/site-packages/transformers/training_args.py:1965: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.
  warnings.warn(
/u/choprahetarth/.conda/envs/scoder_2/lib/python3.10/site-packages/transformers/training_args.py:1965: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.
  warnings.warn(
/u/choprahetarth/.conda/envs/scoder_2/lib/python3.10/site-packages/transformers/training_args.py:1965: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.
  warnings.warn(
/u/choprahetarth/.conda/envs/scoder_2/lib/python3.10/site-packages/transformers/training_args.py:1965: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.
  warnings.warn(
/u/choprahetarth/.conda/envs/scoder_2/lib/python3.10/site-packages/transformers/training_args.py:1965: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.
  warnings.warn(
/u/choprahetarth/.conda/envs/scoder_2/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
  warnings.warn(
/u/choprahetarth/.conda/envs/scoder_2/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
  warnings.warn(
/u/choprahetarth/.conda/envs/scoder_2/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
  warnings.warn(
/u/choprahetarth/.conda/envs/scoder_2/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:355: UserWarning: You passed a `dataset_kwargs` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
  warnings.warn(

Map:   0%|          | 0/13721 [00:00<?, ? examples/s]
Map:   7%|▋         | 1000/13721 [00:00<00:02, 5876.23 examples/s]
Map:  22%|██▏       | 3000/13721 [00:00<00:00, 11371.81 examples/s]
Map:  36%|███▋      | 5000/13721 [00:00<00:00, 13905.30 examples/s]
Map:  51%|█████     | 7000/13721 [00:00<00:00, 15268.73 examples/s]
Map:  66%|██████▌   | 9000/13721 [00:00<00:00, 16194.69 examples/s]
Map:  80%|████████  | 11000/13721 [00:00<00:00, 16708.05 examples/s]
Map:  95%|█████████▍| 13000/13721 [00:00<00:00, 16904.09 examples/s]
Map: 100%|██████████| 13721/13721 [00:00<00:00, 15192.74 examples/s]

Map:   0%|          | 0/1525 [00:00<?, ? examples/s]
Map: 100%|██████████| 1525/1525 [00:00<00:00, 13999.31 examples/s]
Map: 100%|██████████| 1525/1525 [00:00<00:00, 13580.31 examples/s]
/u/choprahetarth/.conda/envs/scoder_2/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:355: UserWarning: You passed a `dataset_kwargs` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
  warnings.warn(
/u/choprahetarth/.conda/envs/scoder_2/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:355: UserWarning: You passed a `dataset_kwargs` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
  warnings.warn(
/u/choprahetarth/.conda/envs/scoder_2/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:355: UserWarning: You passed a `dataset_kwargs` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
  warnings.warn(
WARNING:accelerate.utils.other:Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.

Map:   0%|          | 0/1525 [00:00<?, ? examples/s]
Map:   0%|          | 0/1525 [00:00<?, ? examples/s]
Map:   0%|          | 0/1525 [00:00<?, ? examples/s]
Map:  66%|██████▌   | 1000/1525 [00:00<00:00, 7877.43 examples/s]
Map:  66%|██████▌   | 1000/1525 [00:00<00:00, 7886.76 examples/s]
Map:  66%|██████▌   | 1000/1525 [00:00<00:00, 7237.65 examples/s]
Map: 100%|██████████| 1525/1525 [00:00<00:00, 9214.81 examples/s]

Map: 100%|██████████| 1525/1525 [00:00<00:00, 9260.75 examples/s]

Map: 100%|██████████| 1525/1525 [00:00<00:00, 8651.92 examples/s]
max_steps is given, it will override any value given in num_train_epochs
max_steps is given, it will override any value given in num_train_epochs
max_steps is given, it will override any value given in num_train_epochs
Training...
Training...
Training...
Training...
wandb: Currently logged in as: hetarthvader. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.17.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.2
wandb: Run data is saved locally in /scratch/bbvz/choprahetarth/wandb/run-20240719_141857-qaomwk9t
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run train-starcoder2-15b
wandb: ⭐️ View project at https://wandb.ai/hetarthvader/huggingface
wandb: 🚀 View run at https://wandb.ai/hetarthvader/huggingface/runs/qaomwk9t
/u/choprahetarth/.conda/envs/scoder_2/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/choprahetarth/.conda/envs/scoder_2/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/choprahetarth/.conda/envs/scoder_2/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/choprahetarth/.conda/envs/scoder_2/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
[rank0]:[W reducer.cpp:1389] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank2]:[W reducer.cpp:1389] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank3]:[W reducer.cpp:1389] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank1]:[W reducer.cpp:1389] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
{'loss': 3.5934, 'grad_norm': 0.10963506251573563, 'learning_rate': 1.8e-05, 'epoch': 0.011658408627222384}
{'loss': 3.6109, 'grad_norm': 0.4127148687839508, 'learning_rate': 3.7e-05, 'epoch': 0.023316817254444767}
{'loss': 3.2322, 'grad_norm': 0.455870121717453, 'learning_rate': 5.6999999999999996e-05, 'epoch': 0.034975225881667155}
{'loss': 2.3467, 'grad_norm': 0.48122647404670715, 'learning_rate': 7.6e-05, 'epoch': 0.046633634508889535}
{'loss': 1.6407, 'grad_norm': 0.7563601136207581, 'learning_rate': 9.6e-05, 'epoch': 0.05829204313611192}
{'loss': 1.2837, 'grad_norm': 0.3972342312335968, 'learning_rate': 9.997578415618427e-05, 'epoch': 0.06995045176333431}
{'loss': 1.2558, 'grad_norm': 0.327361524105072, 'learning_rate': 9.987744749025853e-05, 'epoch': 0.08160886039055669}
{'loss': 1.2259, 'grad_norm': 0.36623355746269226, 'learning_rate': 9.970362522226291e-05, 'epoch': 0.09326726901777907}
{'loss': 1.2164, 'grad_norm': 0.44303154945373535, 'learning_rate': 9.94545804185573e-05, 'epoch': 0.10492567764500146}
{'loss': 1.1494, 'grad_norm': 0.37725356221199036, 'learning_rate': 9.913068998890546e-05, 'epoch': 0.11658408627222384}
{'loss': 1.1796, 'grad_norm': 1.0593550205230713, 'learning_rate': 9.873244411605162e-05, 'epoch': 0.12824249489944622}
{'loss': 1.163, 'grad_norm': 0.3409847319126129, 'learning_rate': 9.826044551386744e-05, 'epoch': 0.13990090352666862}
{'loss': 1.0201, 'grad_norm': 0.42495718598365784, 'learning_rate': 9.771540851519179e-05, 'epoch': 0.15155931215389098}
{'loss': 1.0705, 'grad_norm': 1.00526762008667, 'learning_rate': 9.709815799074413e-05, 'epoch': 0.16321772078111338}
{'loss': 0.9813, 'grad_norm': 0.47312256693840027, 'learning_rate': 9.640962810074735e-05, 'epoch': 0.17487612940833577}
{'loss': 1.0004, 'grad_norm': 0.36922675371170044, 'learning_rate': 9.565086088114967e-05, 'epoch': 0.18653453803555814}
{'loss': 0.9817, 'grad_norm': 0.46072161197662354, 'learning_rate': 9.482300466658499e-05, 'epoch': 0.19819294666278053}
{'loss': 1.0167, 'grad_norm': 0.39226582646369934, 'learning_rate': 9.392731235245846e-05, 'epoch': 0.20985135529000293}
{'loss': 0.9737, 'grad_norm': 1.1553282737731934, 'learning_rate': 9.296513949878778e-05, 'epoch': 0.2215097639172253}
{'loss': 0.9733, 'grad_norm': 0.4668451249599457, 'learning_rate': 9.193794227866938e-05, 'epoch': 0.2331681725444477}
{'loss': 0.9651, 'grad_norm': 0.3920031189918518, 'learning_rate': 9.08472752744748e-05, 'epoch': 0.24482658117167005}
{'loss': 0.9733, 'grad_norm': 0.400215208530426, 'learning_rate': 8.969478912511225e-05, 'epoch': 0.25648498979889245}
{'loss': 0.9379, 'grad_norm': 0.42527785897254944, 'learning_rate': 8.848222802791415e-05, 'epoch': 0.26814339842611484}
{'loss': 0.9177, 'grad_norm': 0.3430953919887543, 'learning_rate': 8.721142709893146e-05, 'epoch': 0.27980180705333724}
{'loss': 0.9225, 'grad_norm': 0.36995646357536316, 'learning_rate': 8.588430959562945e-05, 'epoch': 0.29146021568055963}
/u/choprahetarth/.conda/envs/scoder_2/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/u/choprahetarth/.conda/envs/scoder_2/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.954, 'grad_norm': 0.42234116792678833, 'learning_rate': 8.450288400618855e-05, 'epoch': 0.30311862430778197}
{'loss': 0.9535, 'grad_norm': 0.3983096480369568, 'learning_rate': 8.306924100981513e-05, 'epoch': 0.31477703293500436}
{'loss': 0.92, 'grad_norm': 0.43905702233314514, 'learning_rate': 8.158555031266254e-05, 'epoch': 0.32643544156222676}
{'loss': 0.9357, 'grad_norm': 0.5111896395683289, 'learning_rate': 8.005405736415126e-05, 'epoch': 0.33809385018944915}
{'loss': 0.9704, 'grad_norm': 0.5019380450248718, 'learning_rate': 7.847707995865729e-05, 'epoch': 0.34975225881667155}
{'loss': 0.9147, 'grad_norm': 0.4128360450267792, 'learning_rate': 7.685700472771243e-05, 'epoch': 0.3614106674438939}
{'loss': 0.9619, 'grad_norm': 0.458830863237381, 'learning_rate': 7.519628352802463e-05, 'epoch': 0.3730690760711163}
{'loss': 0.9652, 'grad_norm': 0.46739888191223145, 'learning_rate': 7.349742973078547e-05, 'epoch': 0.3847274846983387}
{'loss': 0.9261, 'grad_norm': 0.4011290669441223, 'learning_rate': 7.176301441787992e-05, 'epoch': 0.39638589332556107}
{'loss': 0.8354, 'grad_norm': 0.3815109431743622, 'learning_rate': 6.999566249075588e-05, 'epoch': 0.40804430195278346}
{'loss': 0.911, 'grad_norm': 0.4234287738800049, 'learning_rate': 6.819804869784179e-05, 'epoch': 0.41970271058000586}
{'loss': 0.8532, 'grad_norm': 0.3284512460231781, 'learning_rate': 6.637289358652476e-05, 'epoch': 0.4313611192072282}
{'loss': 0.91, 'grad_norm': 0.4710184931755066, 'learning_rate': 6.452295938581576e-05, 'epoch': 0.4430195278344506}
{'loss': 0.8872, 'grad_norm': 0.3976348638534546, 'learning_rate': 6.265104582593267e-05, 'epoch': 0.454677936461673}
{'loss': 0.8974, 'grad_norm': 0.3820120692253113, 'learning_rate': 6.0759985901128435e-05, 'epoch': 0.4663363450888954}
{'loss': 0.8927, 'grad_norm': 0.44951796531677246, 'learning_rate': 5.8852641582176524e-05, 'epoch': 0.47799475371611777}
{'loss': 0.8725, 'grad_norm': 0.44404101371765137, 'learning_rate': 5.693189948500293e-05, 'epoch': 0.4896531623433401}
{'loss': 0.8826, 'grad_norm': 0.49015647172927856, 'learning_rate': 5.5000666502019335e-05, 'epoch': 0.5013115709705626}
{'loss': 0.8691, 'grad_norm': 0.42986154556274414, 'learning_rate': 5.3061865402769796e-05, 'epoch': 0.5129699795977849}
{'loss': 0.9097, 'grad_norm': 0.6307433843612671, 'learning_rate': 5.111843041054827e-05, 'epoch': 0.5246283882250072}
{'loss': 0.8708, 'grad_norm': 0.49416202306747437, 'learning_rate': 4.917330276168208e-05, 'epoch': 0.5362867968522297}
{'loss': 0.8775, 'grad_norm': 0.3832523226737976, 'learning_rate': 4.72294262542015e-05, 'epoch': 0.547945205479452}
{'loss': 0.9004, 'grad_norm': 0.4722447693347931, 'learning_rate': 4.528974279263255e-05, 'epoch': 0.5596036141066745}
{'loss': 0.9049, 'grad_norm': 0.4284020960330963, 'learning_rate': 4.33571879356553e-05, 'epoch': 0.5712620227338968}
{'loss': 0.9301, 'grad_norm': 0.4306858777999878, 'learning_rate': 4.143468645336632e-05, 'epoch': 0.5829204313611193}
/u/choprahetarth/.conda/envs/scoder_2/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/u/choprahetarth/.conda/envs/scoder_2/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8888, 'grad_norm': 0.405380517244339, 'learning_rate': 3.9525147900868654e-05, 'epoch': 0.5945788399883416}
{'loss': 0.8869, 'grad_norm': 0.38943466544151306, 'learning_rate': 3.763146221488846e-05, 'epoch': 0.6062372486155639}
{'loss': 0.93, 'grad_norm': 0.43279263377189636, 'learning_rate': 3.575649534008271e-05, 'epoch': 0.6178956572427864}
{'loss': 0.8423, 'grad_norm': 0.4715297818183899, 'learning_rate': 3.390308489165681e-05, 'epoch': 0.6295540658700087}
{'loss': 0.8679, 'grad_norm': 0.39031389355659485, 'learning_rate': 3.2074035860856776e-05, 'epoch': 0.6412124744972312}
{'loss': 0.893, 'grad_norm': 0.5461024641990662, 'learning_rate': 3.0272116369835024e-05, 'epoch': 0.6528708831244535}
{'loss': 0.9564, 'grad_norm': 0.5623142719268799, 'learning_rate': 2.8500053482314858e-05, 'epoch': 0.6645292917516759}
{'loss': 0.9017, 'grad_norm': 0.4938538372516632, 'learning_rate': 2.6760529076393527e-05, 'epoch': 0.6761877003788983}
{'loss': 0.9166, 'grad_norm': 0.4020797908306122, 'learning_rate': 2.505617578573016e-05, 'epoch': 0.6878461090061206}
{'loss': 0.8727, 'grad_norm': 0.620500385761261, 'learning_rate': 2.33895730152615e-05, 'epoch': 0.6995045176333431}
{'loss': 0.838, 'grad_norm': 0.4541403353214264, 'learning_rate': 2.1763243037474763e-05, 'epoch': 0.7111629262605654}
{'loss': 0.8953, 'grad_norm': 0.3944789469242096, 'learning_rate': 2.0179647175146414e-05, 'epoch': 0.7228213348877878}
{'loss': 0.866, 'grad_norm': 0.36631855368614197, 'learning_rate': 1.8641182076323148e-05, 'epoch': 0.7344797435150102}
{'loss': 0.8523, 'grad_norm': 0.5273465514183044, 'learning_rate': 1.7150176087183317e-05, 'epoch': 0.7461381521422326}
{'loss': 0.9077, 'grad_norm': 0.3757123351097107, 'learning_rate': 1.570888572826761e-05, 'epoch': 0.757796560769455}
{'loss': 0.88, 'grad_norm': 0.3950696885585785, 'learning_rate': 1.4319492279412388e-05, 'epoch': 0.7694549693966773}
{'loss': 0.8599, 'grad_norm': 0.5652843713760376, 'learning_rate': 1.2984098478553874e-05, 'epoch': 0.7811133780238997}
{'loss': 0.8633, 'grad_norm': 0.4133875370025635, 'learning_rate': 1.1704725339399287e-05, 'epoch': 0.7927717866511221}
{'loss': 0.8876, 'grad_norm': 0.48860886693000793, 'learning_rate': 1.048330909278119e-05, 'epoch': 0.8044301952783445}
{'loss': 0.8514, 'grad_norm': 0.5418506860733032, 'learning_rate': 9.32169825632414e-06, 'epoch': 0.8160886039055669}
{'loss': 0.7895, 'grad_norm': 0.4271267056465149, 'learning_rate': 8.221650836858341e-06, 'epoch': 0.8277470125327893}
{'loss': 0.895, 'grad_norm': 0.40998315811157227, 'learning_rate': 7.184831669814296e-06, 'epoch': 0.8394054211600117}
{'loss': 0.8539, 'grad_norm': 0.5216781497001648, 'learning_rate': 6.212809899625e-06, 'epoch': 0.851063829787234}
{'loss': 0.9002, 'grad_norm': 0.39294111728668213, 'learning_rate': 5.3070566049490234e-06, 'epoch': 0.8627222384144564}
{'loss': 0.8951, 'grad_norm': 0.4488554298877716, 'learning_rate': 4.46894257230846e-06, 'epoch': 0.8743806470416788}
/u/choprahetarth/.conda/envs/scoder_2/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/u/choprahetarth/.conda/envs/scoder_2/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8886, 'grad_norm': 0.37388673424720764, 'learning_rate': 3.6997362215110907e-06, 'epoch': 0.8860390556689012}
{'loss': 0.8938, 'grad_norm': 0.48918023705482483, 'learning_rate': 3.0006016859964904e-06, 'epoch': 0.8976974642961236}
{'loss': 0.8985, 'grad_norm': 0.43280652165412903, 'learning_rate': 2.3725970510115492e-06, 'epoch': 0.909355872923346}
{'loss': 0.8503, 'grad_norm': 0.5050631165504456, 'learning_rate': 1.8166727522814343e-06, 'epoch': 0.9210142815505683}
{'loss': 0.8713, 'grad_norm': 0.41695335507392883, 'learning_rate': 1.333670137599713e-06, 'epoch': 0.9326726901777908}
{'loss': 0.865, 'grad_norm': 0.45297208428382874, 'learning_rate': 9.243201935145108e-07, 'epoch': 0.9443310988050131}
{'loss': 0.8688, 'grad_norm': 0.3891716003417969, 'learning_rate': 5.892424390377626e-07, 'epoch': 0.9559895074322355}
{'loss': 0.8612, 'grad_norm': 0.47395721077919006, 'learning_rate': 3.2894398805177753e-07, 'epoch': 0.9676479160594579}
{'loss': 0.9006, 'grad_norm': 0.4929943382740021, 'learning_rate': 1.4381878183221675e-07, 'epoch': 0.9793063246866802}
{'loss': 0.8196, 'grad_norm': 0.4234371781349182, 'learning_rate': 3.4146992848854695e-08, 'epoch': 0.9909647333139027}
{'train_runtime': 3025.2035, 'train_samples_per_second': 4.535, 'train_steps_per_second': 0.567, 'train_loss': 1.0458239674915726, 'epoch': 0.9997085397843194}
Starcoder2ForCausalLM(
  (model): Starcoder2Model(
    (embed_tokens): Embedding(49152, 6144)
    (layers): ModuleList(
      (0-39): 40 x Starcoder2DecoderLayer(
        (self_attn): Starcoder2SdpaAttention(
          (q_proj): lora.Linear8bitLt(
            (base_layer): Linear8bitLt(in_features=6144, out_features=6144, bias=True)
            (lora_dropout): ModuleDict(
              (default): Identity()
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=6144, out_features=32, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=32, out_features=6144, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
          )
          (k_proj): lora.Linear8bitLt(
            (base_layer): Linear8bitLt(in_features=6144, out_features=512, bias=True)
            (lora_dropout): ModuleDict(
              (default): Identity()
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=6144, out_features=32, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=32, out_features=512, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
          )
          (v_proj): lora.Linear8bitLt(
            (base_layer): Linear8bitLt(in_features=6144, out_features=512, bias=True)
            (lora_dropout): ModuleDict(
              (default): Identity()
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=6144, out_features=32, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=32, out_features=512, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
          )
          (o_proj): lora.Linear8bitLt(
            (base_layer): Linear8bitLt(in_features=6144, out_features=6144, bias=True)
            (lora_dropout): ModuleDict(
              (default): Identity()
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=6144, out_features=32, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=32, out_features=6144, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
          )
          (rotary_emb): Starcoder2RotaryEmbedding()
        )
        (mlp): Starcoder2MLP(
          (c_fc): Linear8bitLt(in_features=6144, out_features=24576, bias=True)
          (c_proj): Linear8bitLt(in_features=24576, out_features=6144, bias=True)
          (act): PytorchGELUTanh()
        )
        (input_layernorm): LayerNorm((6144,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((6144,), eps=1e-05, elementwise_affine=True)
      )
    )
    (norm): LayerNorm((6144,), eps=1e-05, elementwise_affine=True)
  )
  (lm_head): Linear(in_features=6144, out_features=49152, bias=False)
)
Saving the last checkpoint of the model
saving the model peft layer
Starcoder2ForCausalLM(
  (model): Starcoder2Model(
    (embed_tokens): Embedding(49152, 6144)
    (layers): ModuleList(
      (0-39): 40 x Starcoder2DecoderLayer(
        (self_attn): Starcoder2SdpaAttention(
          (q_proj): lora.Linear8bitLt(
            (base_layer): Linear8bitLt(in_features=6144, out_features=6144, bias=True)
            (lora_dropout): ModuleDict(
              (default): Identity()
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=6144, out_features=32, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=32, out_features=6144, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
          )
          (k_proj): lora.Linear8bitLt(
            (base_layer): Linear8bitLt(in_features=6144, out_features=512, bias=True)
            (lora_dropout): ModuleDict(
              (default): Identity()
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=6144, out_features=32, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=32, out_features=512, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
          )
          (v_proj): lora.Linear8bitLt(
            (base_layer): Linear8bitLt(in_features=6144, out_features=512, bias=True)
            (lora_dropout): ModuleDict(
              (default): Identity()
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=6144, out_features=32, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=32, out_features=512, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
          )
          (o_proj): lora.Linear8bitLt(
            (base_layer): Linear8bitLt(in_features=6144, out_features=6144, bias=True)
            (lora_dropout): ModuleDict(
              (default): Identity()
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=6144, out_features=32, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=32, out_features=6144, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
          )
          (rotary_emb): Starcoder2RotaryEmbedding()
        )
        (mlp): Starcoder2MLP(
          (c_fc): Linear8bitLt(in_features=6144, out_features=24576, bias=True)
          (c_proj): Linear8bitLt(in_features=24576, out_features=6144, bias=True)
          (act): PytorchGELUTanh()
        )
        (input_layernorm): LayerNorm((6144,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((6144,), eps=1e-05, elementwise_affine=True)
      )
    )
    (norm): LayerNorm((6144,), eps=1e-05, elementwise_affine=True)
  )
  (lm_head): Linear(in_features=6144, out_features=49152, bias=False)
)
Saving the last checkpoint of the model
saving the model peft layer
Starcoder2ForCausalLM(
  (model): Starcoder2Model(
    (embed_tokens): Embedding(49152, 6144)
    (layers): ModuleList(
      (0-39): 40 x Starcoder2DecoderLayer(
        (self_attn): Starcoder2SdpaAttention(
          (q_proj): lora.Linear8bitLt(
            (base_layer): Linear8bitLt(in_features=6144, out_features=6144, bias=True)
            (lora_dropout): ModuleDict(
              (default): Identity()
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=6144, out_features=32, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=32, out_features=6144, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
          )
          (k_proj): lora.Linear8bitLt(
            (base_layer): Linear8bitLt(in_features=6144, out_features=512, bias=True)
            (lora_dropout): ModuleDict(
              (default): Identity()
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=6144, out_features=32, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=32, out_features=512, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
          )
          (v_proj): lora.Linear8bitLt(
            (base_layer): Linear8bitLt(in_features=6144, out_features=512, bias=True)
            (lora_dropout): ModuleDict(
              (default): Identity()
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=6144, out_features=32, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=32, out_features=512, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
          )
          (o_proj): lora.Linear8bitLt(
            (base_layer): Linear8bitLt(in_features=6144, out_features=6144, bias=True)
            (lora_dropout): ModuleDict(
              (default): Identity()
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=6144, out_features=32, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=32, out_features=6144, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
          )
          (rotary_emb): Starcoder2RotaryEmbedding()
        )
        (mlp): Starcoder2MLP(
          (c_fc): Linear8bitLt(in_features=6144, out_features=24576, bias=True)
          (c_proj): Linear8bitLt(in_features=24576, out_features=6144, bias=True)
          (act): PytorchGELUTanh()
        )
        (input_layernorm): LayerNorm((6144,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((6144,), eps=1e-05, elementwise_affine=True)
      )
    )
    (norm): LayerNorm((6144,), eps=1e-05, elementwise_affine=True)
  )
  (lm_head): Linear(in_features=6144, out_features=49152, bias=False)
)
Saving the last checkpoint of the model
saving the model peft layer
Starcoder2ForCausalLM(
  (model): Starcoder2Model(
    (embed_tokens): Embedding(49152, 6144)
    (layers): ModuleList(
      (0-39): 40 x Starcoder2DecoderLayer(
        (self_attn): Starcoder2SdpaAttention(
          (q_proj): lora.Linear8bitLt(
            (base_layer): Linear8bitLt(in_features=6144, out_features=6144, bias=True)
            (lora_dropout): ModuleDict(
              (default): Identity()
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=6144, out_features=32, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=32, out_features=6144, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
          )
          (k_proj): lora.Linear8bitLt(
            (base_layer): Linear8bitLt(in_features=6144, out_features=512, bias=True)
            (lora_dropout): ModuleDict(
              (default): Identity()
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=6144, out_features=32, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=32, out_features=512, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
          )
          (v_proj): lora.Linear8bitLt(
            (base_layer): Linear8bitLt(in_features=6144, out_features=512, bias=True)
            (lora_dropout): ModuleDict(
              (default): Identity()
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=6144, out_features=32, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=32, out_features=512, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
          )
          (o_proj): lora.Linear8bitLt(
            (base_layer): Linear8bitLt(in_features=6144, out_features=6144, bias=True)
            (lora_dropout): ModuleDict(
              (default): Identity()
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=6144, out_features=32, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=32, out_features=6144, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
          )
          (rotary_emb): Starcoder2RotaryEmbedding()
        )
        (mlp): Starcoder2MLP(
          (c_fc): Linear8bitLt(in_features=6144, out_features=24576, bias=True)
          (c_proj): Linear8bitLt(in_features=24576, out_features=6144, bias=True)
          (act): PytorchGELUTanh()
        )
        (input_layernorm): LayerNorm((6144,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((6144,), eps=1e-05, elementwise_affine=True)
      )
    )
    (norm): LayerNorm((6144,), eps=1e-05, elementwise_affine=True)
  )
  (lm_head): Linear(in_features=6144, out_features=49152, bias=False)
)
Saving the last checkpoint of the model
saving the model peft layer
/u/choprahetarth/.conda/envs/scoder_2/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/u/choprahetarth/.conda/envs/scoder_2/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/u/choprahetarth/.conda/envs/scoder_2/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/u/choprahetarth/.conda/envs/scoder_2/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Deleted all GPU Memory
Merging the Lora layers back
Deleted all GPU Memory
Merging the Lora layers back
Deleted all GPU Memory
Merging the Lora layers back
Deleted all GPU Memory
Merging the Lora layers back

Loading checkpoint shards:   0%|          | 0/14 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/14 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/14 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/14 [00:00<?, ?it/s]
Loading checkpoint shards:   7%|▋         | 1/14 [01:34<20:33, 94.90s/it]
Loading checkpoint shards:   7%|▋         | 1/14 [01:35<20:40, 95.46s/it]
Loading checkpoint shards:   7%|▋         | 1/14 [01:35<20:38, 95.30s/it]
Loading checkpoint shards:   7%|▋         | 1/14 [01:35<20:39, 95.31s/it]
Loading checkpoint shards:  14%|█▍        | 2/14 [02:21<13:19, 66.65s/it]
Loading checkpoint shards:  14%|█▍        | 2/14 [02:22<13:21, 66.83s/it]
Loading checkpoint shards:  14%|█▍        | 2/14 [02:22<13:21, 66.77s/it]
Loading checkpoint shards:  14%|█▍        | 2/14 [02:22<13:21, 66.77s/it]
Loading checkpoint shards:  21%|██▏       | 3/14 [03:01<09:59, 54.48s/it]
Loading checkpoint shards:  21%|██▏       | 3/14 [03:02<10:00, 54.60s/it]
Loading checkpoint shards:  21%|██▏       | 3/14 [03:02<10:00, 54.64s/it]
Loading checkpoint shards:  21%|██▏       | 3/14 [03:02<10:01, 54.69s/it]
Loading checkpoint shards:  29%|██▊       | 4/14 [03:29<07:18, 43.86s/it]
Loading checkpoint shards:  29%|██▊       | 4/14 [03:29<07:19, 43.93s/it]
Loading checkpoint shards:  29%|██▊       | 4/14 [03:29<07:19, 43.95s/it]
Loading checkpoint shards:  29%|██▊       | 4/14 [03:29<07:19, 43.93s/it]
Loading checkpoint shards:  36%|███▌      | 5/14 [03:56<05:39, 37.77s/it]
Loading checkpoint shards:  36%|███▌      | 5/14 [03:56<05:40, 37.84s/it]
Loading checkpoint shards:  36%|███▌      | 5/14 [03:56<05:40, 37.79s/it]
Loading checkpoint shards:  36%|███▌      | 5/14 [03:56<05:40, 37.78s/it]
Loading checkpoint shards:  43%|████▎     | 6/14 [04:50<05:47, 43.38s/it]
Loading checkpoint shards:  43%|████▎     | 6/14 [04:51<05:47, 43.44s/it]
Loading checkpoint shards:  43%|████▎     | 6/14 [04:51<05:47, 43.45s/it]
Loading checkpoint shards:  43%|████▎     | 6/14 [04:51<05:47, 43.43s/it]
Loading checkpoint shards:  50%|█████     | 7/14 [05:02<03:51, 33.06s/it]
Loading checkpoint shards:  50%|█████     | 7/14 [05:02<03:51, 33.12s/it]
Loading checkpoint shards:  50%|█████     | 7/14 [05:02<03:51, 33.10s/it]
Loading checkpoint shards:  50%|█████     | 7/14 [05:02<03:51, 33.09s/it]
Loading checkpoint shards:  57%|█████▋    | 8/14 [05:05<02:20, 23.43s/it]
Loading checkpoint shards:  57%|█████▋    | 8/14 [05:05<02:20, 23.48s/it]
Loading checkpoint shards:  57%|█████▋    | 8/14 [05:05<02:20, 23.46s/it]
Loading checkpoint shards:  57%|█████▋    | 8/14 [05:05<02:20, 23.46s/it]
Loading checkpoint shards:  64%|██████▍   | 9/14 [05:10<01:28, 17.70s/it]
Loading checkpoint shards:  64%|██████▍   | 9/14 [05:10<01:28, 17.72s/it]
Loading checkpoint shards:  64%|██████▍   | 9/14 [05:10<01:28, 17.70s/it]
Loading checkpoint shards:  64%|██████▍   | 9/14 [05:10<01:28, 17.70s/it]
Loading checkpoint shards:  71%|███████▏  | 10/14 [05:13<00:52, 13.13s/it]
Loading checkpoint shards:  71%|███████▏  | 10/14 [05:13<00:52, 13.14s/it]
Loading checkpoint shards:  71%|███████▏  | 10/14 [05:13<00:52, 13.13s/it]
Loading checkpoint shards:  71%|███████▏  | 10/14 [05:13<00:52, 13.12s/it]
Loading checkpoint shards:  79%|███████▊  | 11/14 [06:02<01:11, 23.95s/it]
Loading checkpoint shards:  79%|███████▊  | 11/14 [06:02<01:12, 24.01s/it]
Loading checkpoint shards:  79%|███████▊  | 11/14 [06:02<01:12, 24.04s/it]
Loading checkpoint shards:  79%|███████▊  | 11/14 [06:02<01:12, 24.03s/it]
Loading checkpoint shards:  86%|████████▌ | 12/14 [06:59<01:08, 34.09s/it]
Loading checkpoint shards:  86%|████████▌ | 12/14 [06:59<01:08, 34.16s/it]
Loading checkpoint shards:  86%|████████▌ | 12/14 [06:59<01:08, 34.15s/it]
Loading checkpoint shards:  86%|████████▌ | 12/14 [06:59<01:08, 34.14s/it]
Loading checkpoint shards:  93%|█████████▎| 13/14 [07:28<00:32, 32.72s/it]
Loading checkpoint shards:  93%|█████████▎| 13/14 [07:29<00:32, 32.75s/it]
Loading checkpoint shards:  93%|█████████▎| 13/14 [07:29<00:32, 32.74s/it]
Loading checkpoint shards:  93%|█████████▎| 13/14 [07:29<00:32, 32.74s/it]
Loading checkpoint shards: 100%|██████████| 14/14 [09:07<00:00, 52.55s/it]
Loading checkpoint shards: 100%|██████████| 14/14 [09:07<00:00, 39.10s/it]
Loading PEFT model...

Loading checkpoint shards: 100%|██████████| 14/14 [09:07<00:00, 52.63s/it]
Loading checkpoint shards: 100%|██████████| 14/14 [09:07<00:00, 39.12s/it]

Loading checkpoint shards: 100%|██████████| 14/14 [09:07<00:00, 52.64s/it]
Loading checkpoint shards: 100%|██████████| 14/14 [09:07<00:00, 39.13s/it]

Loading checkpoint shards: 100%|██████████| 14/14 [09:07<00:00, 52.70s/it]
Loading checkpoint shards: 100%|██████████| 14/14 [09:07<00:00, 39.12s/it]
Loading PEFT model...
Loading PEFT model...
Loading PEFT model...
Merging and unloading model...
Loading tokenizer...
Saving pretrained model and tokenizer...
Merging and unloading model...
Merging and unloading model...
Merging and unloading model...
Loading tokenizer...
Loading tokenizer...
Loading tokenizer...
Saving pretrained model and tokenizer...
Saving pretrained model and tokenizer...
Saving pretrained model and tokenizer...
Training Done! 💥
Training Done! 💥
Training Done! 💥
Training Done! 💥
wandb: - 0.009 MB of 0.009 MB uploaded
wandb: \ 0.009 MB of 0.044 MB uploaded
wandb: 
wandb: Run history:
wandb:         train/epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████
wandb:   train/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:     train/grad_norm ▁▄▆▃▃█▃▃▃▄▃▃▃▄▃▄▃▃▃▄▃▄▄▃▃▄▄▃▄▃▃▄▄▃▃▃▃▃▃▃
wandb: train/learning_rate ▂▅███████▇▇▇▇▇▆▆▆▆▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁
wandb:          train/loss █▇▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:               total_flos 9.51910638068695e+16
wandb:              train/epoch 0.99971
wandb:        train/global_step 1715
wandb:          train/grad_norm 0.42344
wandb:      train/learning_rate 0.0
wandb:               train/loss 0.8196
wandb:               train_loss 1.04582
wandb:            train_runtime 3025.2035
wandb: train_samples_per_second 4.535
wandb:   train_steps_per_second 0.567
wandb: 
wandb: 🚀 View run train-starcoder2-15b at: https://wandb.ai/hetarthvader/huggingface/runs/qaomwk9t
wandb: ⭐️ View project at: https://wandb.ai/hetarthvader/huggingface
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/bbvz/choprahetarth/wandb/run-20240719_141857-qaomwk9t/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.



srun --unbuffered --account=bbvz-delta-gpu \
python -m torch.distributed.run \
--nproc_per_node=4 \
finetune.py \
--model_id="bigcode/starcoder2-15b" \
--dataset_name="/u/choprahetarth/all_files/data/train_ftdata-new-small.json" \
--max_seq_length 1024 \
--max_steps 1715 \
--size_valid_set 1525 \
--micro_batch_size 1 \
--gradient_accumulation_steps 2 \
--weight_decay 0.05 \
--fp16 True \
--lora_rank 32 \
--learning_rate 1e-4 \
--lr_scheduler_type="cosine" \
--warmup_steps 100 \
--seed 1234 \
--output_dir="//scratch/bbvz/choprahetarth/starcoder2_script/bigger_experiments/starcoder2-15b" \
--push_to_hub False \
--save_freq 20
