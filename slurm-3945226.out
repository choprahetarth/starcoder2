Running "module reset". Resetting modules to system default. The following $MODULEPATH directories have been removed: None
WARNING:__main__:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/u/choprahetarth/all_files/starcoder2/u/choprahetarth/all_files/starcoder2

/u/choprahetarth/all_files/starcoder2
/u/choprahetarth/all_files/starcoder2
I am loading LORA
Loading BNB
Loading Model
I am loading LORA
Loading BNB
Loading Model
I am loading LORA
Loading BNB
Loading Model
I am loading LORA
Loading BNB
Loading Model
loading configuration file config.json from cache at /scratch/bbvz/choprahetarth/hub/models--meta-llama--CodeLlama-7b-hf/snapshots/b462c3c99b077d341db691ec780a33156f3c1472/config.json
Model config LlamaConfig {
  "_name_or_path": "meta-llama/CodeLlama-7b-hf",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 16384,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.41.2",
  "use_cache": true,
  "vocab_size": 32016
}

loading configuration file config.json from cache at /scratch/bbvz/choprahetarth/hub/models--meta-llama--CodeLlama-7b-hf/snapshots/b462c3c99b077d341db691ec780a33156f3c1472/config.json
Model config LlamaConfig {
  "_name_or_path": "meta-llama/CodeLlama-7b-hf",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 16384,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.41.2",
  "use_cache": true,
  "vocab_size": 32016
}

Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in 8-bit or 4-bit. Pass your own torch_dtype to specify the dtype of the remaining non-linear layers or pass torch_dtype=torch.float16 to remove this warning.
Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in 8-bit or 4-bit. Pass your own torch_dtype to specify the dtype of the remaining non-linear layers or pass torch_dtype=torch.float16 to remove this warning.
loading configuration file config.json from cache at /scratch/bbvz/choprahetarth/hub/models--meta-llama--CodeLlama-7b-hf/snapshots/b462c3c99b077d341db691ec780a33156f3c1472/config.json
Model config LlamaConfig {
  "_name_or_path": "meta-llama/CodeLlama-7b-hf",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 16384,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.41.2",
  "use_cache": true,
  "vocab_size": 32016
}

Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in 8-bit or 4-bit. Pass your own torch_dtype to specify the dtype of the remaining non-linear layers or pass torch_dtype=torch.float16 to remove this warning.
loading configuration file config.json from cache at /scratch/bbvz/choprahetarth/hub/models--meta-llama--CodeLlama-7b-hf/snapshots/b462c3c99b077d341db691ec780a33156f3c1472/config.json
Model config LlamaConfig {
  "_name_or_path": "meta-llama/CodeLlama-7b-hf",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 16384,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.41.2",
  "use_cache": true,
  "vocab_size": 32016
}

loading weights file model.safetensors from cache at /scratch/bbvz/choprahetarth/hub/models--meta-llama--CodeLlama-7b-hf/snapshots/b462c3c99b077d341db691ec780a33156f3c1472/model.safetensors.index.json
Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]loading weights file model.safetensors from cache at /scratch/bbvz/choprahetarth/hub/models--meta-llama--CodeLlama-7b-hf/snapshots/b462c3c99b077d341db691ec780a33156f3c1472/model.safetensors.index.json
Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]loading weights file model.safetensors from cache at /scratch/bbvz/choprahetarth/hub/models--meta-llama--CodeLlama-7b-hf/snapshots/b462c3c99b077d341db691ec780a33156f3c1472/model.safetensors.index.json
Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in 8-bit or 4-bit. Pass your own torch_dtype to specify the dtype of the remaining non-linear layers or pass torch_dtype=torch.float16 to remove this warning.
loading weights file model.safetensors from cache at /scratch/bbvz/choprahetarth/hub/models--meta-llama--CodeLlama-7b-hf/snapshots/b462c3c99b077d341db691ec780a33156f3c1472/model.safetensors.index.json
Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]Downloading shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:57<00:57, 57.33s/it]Downloading shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:57<00:57, 57.31s/it]Downloading shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:57<00:57, 57.27s/it]Downloading shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:57<00:57, 57.39s/it]Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [01:20<00:00, 37.30s/it]Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [01:20<00:00, 40.31s/it]
Instantiating LlamaForCausalLM model under default dtype torch.float16.
Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2
}

Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [01:20<00:00, 37.28s/it]Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [01:20<00:00, 40.30s/it]
Instantiating LlamaForCausalLM model under default dtype torch.float16.
Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2
}

Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [01:20<00:00, 37.31s/it]Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [01:20<00:00, 40.30s/it]
Instantiating LlamaForCausalLM model under default dtype torch.float16.
Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2
}

Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [01:20<00:00, 37.36s/it]Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [01:20<00:00, 40.35s/it]
Instantiating LlamaForCausalLM model under default dtype torch.float16.
Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2
}

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:09<00:09,  9.02s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:09<00:09,  9.37s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:09<00:09,  9.39s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:09<00:09,  9.41s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:12<00:00,  5.53s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:12<00:00,  6.05s/it]
All model checkpoint weights were used when initializing LlamaForCausalLM.

All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/CodeLlama-7b-hf.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
loading configuration file generation_config.json from cache at /scratch/bbvz/choprahetarth/hub/models--meta-llama--CodeLlama-7b-hf/snapshots/b462c3c99b077d341db691ec780a33156f3c1472/generation_config.json
Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2
}

Model Loaded
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:12<00:00,  5.73s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:12<00:00,  6.27s/it]
All model checkpoint weights were used when initializing LlamaForCausalLM.

All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/CodeLlama-7b-hf.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:12<00:00,  5.72s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:12<00:00,  6.27s/it]
All model checkpoint weights were used when initializing LlamaForCausalLM.

All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/CodeLlama-7b-hf.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:12<00:00,  5.75s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:12<00:00,  6.30s/it]
All model checkpoint weights were used when initializing LlamaForCausalLM.

All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/CodeLlama-7b-hf.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
loading configuration file generation_config.json from cache at /scratch/bbvz/choprahetarth/hub/models--meta-llama--CodeLlama-7b-hf/snapshots/b462c3c99b077d341db691ec780a33156f3c1472/generation_config.json
Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2
}

loading configuration file generation_config.json from cache at /scratch/bbvz/choprahetarth/hub/models--meta-llama--CodeLlama-7b-hf/snapshots/b462c3c99b077d341db691ec780a33156f3c1472/generation_config.json
Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2
}

Model Loaded
loading configuration file generation_config.json from cache at /scratch/bbvz/choprahetarth/hub/models--meta-llama--CodeLlama-7b-hf/snapshots/b462c3c99b077d341db691ec780a33156f3c1472/generation_config.json
Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2
}

Model Loaded
Model Loaded
Setting num_proc from 64 back to 1 for the train split to disable multiprocessing as it only contains one shard.
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 15246 examples [00:00, 68119.35 examples/s]Generating train split: 15246 examples [00:00, 67575.57 examples/s]
Map:   0%|          | 0/15246 [00:00<?, ? examples/s]Map:   0%|          | 0/15246 [00:00<?, ? examples/s]Map:   0%|          | 0/15246 [00:00<?, ? examples/s]Map:   0%|          | 0/15246 [00:00<?, ? examples/s]Map:  14%|â–ˆâ–Ž        | 2060/15246 [00:00<00:00, 20474.90 examples/s]Map:  13%|â–ˆâ–Ž        | 2000/15246 [00:00<00:00, 19779.32 examples/s]Map:  14%|â–ˆâ–        | 2104/15246 [00:00<00:00, 20761.29 examples/s]Map:  13%|â–ˆâ–Ž        | 1963/15246 [00:00<00:00, 19495.50 examples/s]Map:  29%|â–ˆâ–ˆâ–Š       | 4375/15246 [00:00<00:00, 22039.99 examples/s]Map:  28%|â–ˆâ–ˆâ–Š       | 4297/15246 [00:00<00:00, 21640.37 examples/s]Map:  28%|â–ˆâ–ˆâ–Š       | 4297/15246 [00:00<00:00, 21436.69 examples/s]Map:  28%|â–ˆâ–ˆâ–Š       | 4229/15246 [00:00<00:00, 21346.93 examples/s]Map:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 6698/15246 [00:00<00:00, 22580.45 examples/s]Map:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 6615/15246 [00:00<00:00, 22339.61 examples/s]Map:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 6550/15246 [00:00<00:00, 21929.69 examples/s]Map:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 6550/15246 [00:00<00:00, 22193.78 examples/s]Map:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 8969/15246 [00:00<00:00, 22630.85 examples/s]Map:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 8919/15246 [00:00<00:00, 22612.26 examples/s]Map:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 8796/15246 [00:00<00:00, 22134.15 examples/s]Map:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 8838/15246 [00:00<00:00, 22460.04 examples/s]Map:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 11216/15246 [00:00<00:00, 22738.98 examples/s]Map:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 11039/15246 [00:00<00:00, 22236.49 examples/s]Map:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 12386/15246 [00:00<00:00, 22693.06 examples/s]Map:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 11118/15246 [00:00<00:00, 22577.52 examples/s]Map:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 13519/15246 [00:00<00:00, 22834.89 examples/s]Map:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 13283/15246 [00:00<00:00, 22302.55 examples/s]Map:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 14696/15246 [00:00<00:00, 22814.75 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15246/15246 [00:00<00:00, 22395.69 examples/s]
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15246/15246 [00:00<00:00, 22280.79 examples/s]
Map:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 13382/15246 [00:00<00:00, 22595.95 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15246/15246 [00:00<00:00, 21923.97 examples/s]
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15246/15246 [00:00<00:00, 22135.58 examples/s]
Sample from the training dataset:  {'prompt': 'Given this Ansible Name Field, please generate the ansible task. name: Ensure fail2ban service is enabled and started', 'completion': 'service:\n  name: fail2ban\n  state: started\n  enabled: true\nwhen: centos_base_fail2ban_configuration|bool\n'}
PyTorch: setting up devices
/u/choprahetarth/.conda/envs/scoder_2/lib/python3.12/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.

Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
  warnings.warn(message, FutureWarning)
PyTorch: setting up devices
Sample from the training dataset:  {'prompt': 'Given this Ansible Name Field, please generate the ansible task. name: Ensure fail2ban service is enabled and started', 'completion': 'service:\n  name: fail2ban\n  state: started\n  enabled: true\nwhen: centos_base_fail2ban_configuration|bool\n'}
PyTorch: setting up devices
/u/choprahetarth/.conda/envs/scoder_2/lib/python3.12/site-packages/transformers/training_args.py:1965: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
  warnings.warn(
/u/choprahetarth/.conda/envs/scoder_2/lib/python3.12/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.

Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
  warnings.warn(message, FutureWarning)
PyTorch: setting up devices
Sample from the training dataset:  {'prompt': 'Given this Ansible Name Field, please generate the ansible task. name: Ensure fail2ban service is enabled and started', 'completion': 'service:\n  name: fail2ban\n  state: started\n  enabled: true\nwhen: centos_base_fail2ban_configuration|bool\n'}
PyTorch: setting up devices
Sample from the training dataset:  {'prompt': 'Given this Ansible Name Field, please generate the ansible task. name: Ensure fail2ban service is enabled and started', 'completion': 'service:\n  name: fail2ban\n  state: started\n  enabled: true\nwhen: centos_base_fail2ban_configuration|bool\n'}
PyTorch: setting up devices
/u/choprahetarth/.conda/envs/scoder_2/lib/python3.12/site-packages/transformers/training_args.py:1965: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
  warnings.warn(
PyTorch: setting up devices
PyTorch: setting up devices
/u/choprahetarth/.conda/envs/scoder_2/lib/python3.12/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.

Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
  warnings.warn(message, FutureWarning)
PyTorch: setting up devices
/u/choprahetarth/.conda/envs/scoder_2/lib/python3.12/site-packages/transformers/training_args.py:1965: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
  warnings.warn(
/u/choprahetarth/.conda/envs/scoder_2/lib/python3.12/site-packages/transformers/training_args.py:1965: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
  warnings.warn(
/u/choprahetarth/.conda/envs/scoder_2/lib/python3.12/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.

Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
  warnings.warn(message, FutureWarning)
PyTorch: setting up devices
/u/choprahetarth/.conda/envs/scoder_2/lib/python3.12/site-packages/transformers/training_args.py:1965: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
  warnings.warn(
PyTorch: setting up devices
/u/choprahetarth/.conda/envs/scoder_2/lib/python3.12/site-packages/transformers/training_args.py:1965: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
  warnings.warn(
PyTorch: setting up devices
/u/choprahetarth/.conda/envs/scoder_2/lib/python3.12/site-packages/transformers/training_args.py:1965: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
  warnings.warn(
/u/choprahetarth/.conda/envs/scoder_2/lib/python3.12/site-packages/transformers/training_args.py:1965: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.
  warnings.warn(
loading file tokenizer.model from cache at /scratch/bbvz/choprahetarth/hub/models--meta-llama--CodeLlama-7b-hf/snapshots/b462c3c99b077d341db691ec780a33156f3c1472/tokenizer.model
loading file tokenizer.json from cache at /scratch/bbvz/choprahetarth/hub/models--meta-llama--CodeLlama-7b-hf/snapshots/b462c3c99b077d341db691ec780a33156f3c1472/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /scratch/bbvz/choprahetarth/hub/models--meta-llama--CodeLlama-7b-hf/snapshots/b462c3c99b077d341db691ec780a33156f3c1472/special_tokens_map.json
loading file tokenizer_config.json from cache at /scratch/bbvz/choprahetarth/hub/models--meta-llama--CodeLlama-7b-hf/snapshots/b462c3c99b077d341db691ec780a33156f3c1472/tokenizer_config.json
loading file tokenizer.model from cache at /scratch/bbvz/choprahetarth/hub/models--meta-llama--CodeLlama-7b-hf/snapshots/b462c3c99b077d341db691ec780a33156f3c1472/tokenizer.model
loading file tokenizer.json from cache at /scratch/bbvz/choprahetarth/hub/models--meta-llama--CodeLlama-7b-hf/snapshots/b462c3c99b077d341db691ec780a33156f3c1472/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /scratch/bbvz/choprahetarth/hub/models--meta-llama--CodeLlama-7b-hf/snapshots/b462c3c99b077d341db691ec780a33156f3c1472/special_tokens_map.json
loading file tokenizer_config.json from cache at /scratch/bbvz/choprahetarth/hub/models--meta-llama--CodeLlama-7b-hf/snapshots/b462c3c99b077d341db691ec780a33156f3c1472/tokenizer_config.json
loading file tokenizer.model from cache at /scratch/bbvz/choprahetarth/hub/models--meta-llama--CodeLlama-7b-hf/snapshots/b462c3c99b077d341db691ec780a33156f3c1472/tokenizer.model
loading file tokenizer.json from cache at /scratch/bbvz/choprahetarth/hub/models--meta-llama--CodeLlama-7b-hf/snapshots/b462c3c99b077d341db691ec780a33156f3c1472/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /scratch/bbvz/choprahetarth/hub/models--meta-llama--CodeLlama-7b-hf/snapshots/b462c3c99b077d341db691ec780a33156f3c1472/special_tokens_map.json
loading file tokenizer_config.json from cache at /scratch/bbvz/choprahetarth/hub/models--meta-llama--CodeLlama-7b-hf/snapshots/b462c3c99b077d341db691ec780a33156f3c1472/tokenizer_config.json
/u/choprahetarth/.conda/envs/scoder_2/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
  warnings.warn(
/u/choprahetarth/.conda/envs/scoder_2/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
  warnings.warn(
/u/choprahetarth/.conda/envs/scoder_2/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
  warnings.warn(
/u/choprahetarth/.conda/envs/scoder_2/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:355: UserWarning: You passed a `dataset_kwargs` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
  warnings.warn(
Map:   0%|          | 0/13721 [00:00<?, ? examples/s]loading file tokenizer.model from cache at /scratch/bbvz/choprahetarth/hub/models--meta-llama--CodeLlama-7b-hf/snapshots/b462c3c99b077d341db691ec780a33156f3c1472/tokenizer.model
loading file tokenizer.json from cache at /scratch/bbvz/choprahetarth/hub/models--meta-llama--CodeLlama-7b-hf/snapshots/b462c3c99b077d341db691ec780a33156f3c1472/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /scratch/bbvz/choprahetarth/hub/models--meta-llama--CodeLlama-7b-hf/snapshots/b462c3c99b077d341db691ec780a33156f3c1472/special_tokens_map.json
loading file tokenizer_config.json from cache at /scratch/bbvz/choprahetarth/hub/models--meta-llama--CodeLlama-7b-hf/snapshots/b462c3c99b077d341db691ec780a33156f3c1472/tokenizer_config.json
No chat template is set for this tokenizer, falling back to a default class-level template. This is very error-prone, because models are often trained with templates different from the class default! Default chat templates are a legacy feature and will be removed in Transformers v4.43, at which point any code depending on them will stop working. We recommend setting a valid chat template before then to ensure that this model continues working without issues.
/u/choprahetarth/.conda/envs/scoder_2/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
  warnings.warn(
Map:   7%|â–‹         | 1000/13721 [00:00<00:04, 3033.98 examples/s]Map:  22%|â–ˆâ–ˆâ–       | 3000/13721 [00:00<00:01, 6725.54 examples/s]Map:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 5000/13721 [00:00<00:00, 8853.05 examples/s]Map:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 7000/13721 [00:00<00:00, 10084.65 examples/s]Map:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 9000/13721 [00:00<00:00, 10936.03 examples/s]Map:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 11000/13721 [00:01<00:00, 11485.64 examples/s]Map:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 13000/13721 [00:01<00:00, 11910.18 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13721/13721 [00:01<00:00, 10154.71 examples/s]
Map:   0%|          | 0/1525 [00:00<?, ? examples/s]Map:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1000/1525 [00:00<00:00, 9058.70 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1525/1525 [00:00<00:00, 9836.32 examples/s]
/u/choprahetarth/.conda/envs/scoder_2/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:355: UserWarning: You passed a `dataset_kwargs` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
  warnings.warn(
/u/choprahetarth/.conda/envs/scoder_2/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:397: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.
  warnings.warn(
/u/choprahetarth/.conda/envs/scoder_2/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:355: UserWarning: You passed a `dataset_kwargs` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
  warnings.warn(
/u/choprahetarth/.conda/envs/scoder_2/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:355: UserWarning: You passed a `dataset_kwargs` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
  warnings.warn(
WARNING:accelerate.utils.other:Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Map:   0%|          | 0/1525 [00:00<?, ? examples/s]Map:   0%|          | 0/1525 [00:00<?, ? examples/s]Map:   0%|          | 0/1525 [00:00<?, ? examples/s]No chat template is set for this tokenizer, falling back to a default class-level template. This is very error-prone, because models are often trained with templates different from the class default! Default chat templates are a legacy feature and will be removed in Transformers v4.43, at which point any code depending on them will stop working. We recommend setting a valid chat template before then to ensure that this model continues working without issues.
No chat template is set for this tokenizer, falling back to a default class-level template. This is very error-prone, because models are often trained with templates different from the class default! Default chat templates are a legacy feature and will be removed in Transformers v4.43, at which point any code depending on them will stop working. We recommend setting a valid chat template before then to ensure that this model continues working without issues.
No chat template is set for this tokenizer, falling back to a default class-level template. This is very error-prone, because models are often trained with templates different from the class default! Default chat templates are a legacy feature and will be removed in Transformers v4.43, at which point any code depending on them will stop working. We recommend setting a valid chat template before then to ensure that this model continues working without issues.
Map:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1000/1525 [00:00<00:00, 5941.83 examples/s]Map:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1000/1525 [00:00<00:00, 5512.81 examples/s]Map:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1000/1525 [00:00<00:00, 5512.90 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1525/1525 [00:00<00:00, 7000.12 examples/s]
/u/choprahetarth/.conda/envs/scoder_2/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:397: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.
  warnings.warn(
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1525/1525 [00:00<00:00, 6510.24 examples/s]
/u/choprahetarth/.conda/envs/scoder_2/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:397: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.
  warnings.warn(
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1525/1525 [00:00<00:00, 6261.53 examples/s]
/u/choprahetarth/.conda/envs/scoder_2/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:397: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.
  warnings.warn(
max_steps is given, it will override any value given in num_train_epochs
max_steps is given, it will override any value given in num_train_epochs
max_steps is given, it will override any value given in num_train_epochs
Training...
Training...
Training...
max_steps is given, it will override any value given in num_train_epochs
Using auto half precision backend
Training...
***** Running training *****
  Num examples = 13,721
  Num Epochs = 1
  Instantaneous batch size per device = 4
  Total train batch size (w. parallel, distributed & accumulation) = 16
  Gradient Accumulation steps = 1
  Total optimization steps = 858
  Number of trainable parameters = 39,976,960
Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
wandb: Currently logged in as: hetarthvader. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.17.2
wandb: Run data is saved locally in /scratch/bbvz/choprahetarth/wandb/run-20240622_182610-rnljyv1a
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run train-CodeLlama-7b-hf
wandb: â­ï¸ View project at https://wandb.ai/hetarthvader/huggingface
wandb: ðŸš€ View run at https://wandb.ai/hetarthvader/huggingface/runs/rnljyv1a
  0%|          | 0/858 [00:00<?, ?it/s]/u/choprahetarth/.conda/envs/scoder_2/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/choprahetarth/.conda/envs/scoder_2/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/choprahetarth/.conda/envs/scoder_2/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/choprahetarth/.conda/envs/scoder_2/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
[rank3]:[W reducer.cpp:1389] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank1]:[W reducer.cpp:1389] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank0]:[W reducer.cpp:1389] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank2]:[W reducer.cpp:1389] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
{'loss': 2.7291, 'grad_norm': 0.3331668972969055, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.01}
  0%|          | 1/858 [00:04<1:07:57,  4.76s/it]  0%|          | 2/858 [00:05<37:30,  2.63s/it]    0%|          | 3/858 [00:08<34:43,  2.44s/it]  0%|          | 4/858 [00:09<28:00,  1.97s/it]  1%|          | 5/858 [00:12<32:33,  2.29s/it]  1%|          | 6/858 [00:13<29:17,  2.06s/it]  1%|          | 7/858 [00:14<25:00,  1.76s/it]  1%|          | 8/858 [00:15<21:29,  1.52s/it]  1%|          | 9/858 [00:17<20:46,  1.47s/it]  1%|          | 10/858 [00:19<22:10,  1.57s/it]                                                  1%|          | 10/858 [00:19<22:10,  1.57s/it]  1%|â–         | 11/858 [00:21<24:02,  1.70s/it]  1%|â–         | 12/858 [00:22<23:00,  1.63s/it]  2%|â–         | 13/858 [00:23<21:02,  1.49s/it]  2%|â–         | 14/858 [00:26<26:54,  1.91s/it]  2%|â–         | 15/858 [00:27<23:52,  1.70s/it]  2%|â–         | 16/858 [00:28<20:34,  1.47s/it]  2%|â–         | 17/858 [00:29<18:48,  1.34s/it]  2%|â–         | 18/858 [00:30<17:40,  1.26s/it]  2%|â–         | 19/858 [00:3{'loss': 2.7485, 'grad_norm': 0.6500319838523865, 'learning_rate': 3.6e-05, 'epoch': 0.02}
{'loss': 2.4446, 'grad_norm': 0.6831644773483276, 'learning_rate': 5.6000000000000006e-05, 'epoch': 0.03}
2<17:20,  1.24s/it]  2%|â–         | 20/858 [00:34<20:59,  1.50s/it]                                                  2%|â–         | 20/858 [00:34<20:59,  1.50s/it]  2%|â–         | 21/858 [00:35<21:21,  1.53s/it]  3%|â–Ž         | 22/858 [00:37<20:45,  1.49s/it]  3%|â–Ž         | 23/858 [00:38<19:37,  1.41s/it]  3%|â–Ž         | 24/858 [00:39<17:40,  1.27s/it]  3%|â–Ž         | 25/858 [00:42<23:56,  1.72s/it]  3%|â–Ž         | 26/858 [00:43<22:58,  1.66s/it]  3%|â–Ž         | 27/858 [00:44<20:36,  1.49s/it]  3%|â–Ž         | 28/858 [00:45<19:28,  1.41s/it]  3%|â–Ž         | 29/858 [00:47<18:49,  1.36s/it]  3%|â–Ž         | 30/858 [00:48<18:05,  1.31s/it]                                                  3%|â–Ž         | 30/858 [00:48<18:05,  1.31s/it]  4%|â–Ž         | 31/858 [00:49<16:47,  1.22s/it]  4%|â–Ž         | 32/858 [00:50<15:40,  1.14s/it]  4%|â–         | 33/858 [00:52<18:16,  1.33s/it]  4%|â–         | 34/858 [00:53<18:08,  1.32s/it]  4%|â–         | 35/858 [00:54<16:5{'loss': 1.847, 'grad_norm': 0.6841182112693787, 'learning_rate': 7.6e-05, 'epoch': 0.05}
{'loss': 1.2, 'grad_norm': 0.226673886179924, 'learning_rate': 9.6e-05, 'epoch': 0.06}
9,  1.24s/it]  4%|â–         | 36/858 [00:57<24:21,  1.78s/it]  4%|â–         | 37/858 [00:58<22:33,  1.65s/it]  4%|â–         | 38/858 [01:00<20:40,  1.51s/it]  5%|â–         | 39/858 [01:01<18:25,  1.35s/it]  5%|â–         | 40/858 [01:03<22:45,  1.67s/it]                                                  5%|â–         | 40/858 [01:03<22:45,  1.67s/it]  5%|â–         | 41/858 [01:04<21:15,  1.56s/it]  5%|â–         | 42/858 [01:06<20:53,  1.54s/it]  5%|â–Œ         | 43/858 [01:07<18:52,  1.39s/it]  5%|â–Œ         | 44/858 [01:08<17:21,  1.28s/it]  5%|â–Œ         | 45/858 [01:09<18:41,  1.38s/it]  5%|â–Œ         | 46/858 [01:11<18:09,  1.34s/it]  5%|â–Œ         | 47/858 [01:12<17:39,  1.31s/it]  6%|â–Œ         | 48/858 [01:13<16:43,  1.24s/it]  6%|â–Œ         | 49/858 [01:15<19:09,  1.42s/it]  6%|â–Œ         | 50/858 [01:17<20:24,  1.52s/it]                                                  6%|â–Œ         | 50/858 [01:17<20:24,  1.52s/it]  6%|â–Œ         | 51/858 [01:18<18:54,  1.{'loss': 0.9928, 'grad_norm': 0.3191755414009094, 'learning_rate': 0.000116, 'epoch': 0.07}
41s/it]  6%|â–Œ         | 52/858 [01:19<17:33,  1.31s/it]  6%|â–Œ         | 53/858 [01:20<16:08,  1.20s/it]  6%|â–‹         | 54/858 [01:21<15:46,  1.18s/it]  6%|â–‹         | 55/858 [01:23<18:45,  1.40s/it]  7%|â–‹         | 56/858 [01:24<17:55,  1.34s/it]  7%|â–‹         | 57/858 [01:25<17:47,  1.33s/it]  7%|â–‹         | 58/858 [01:27<18:07,  1.36s/it]  7%|â–‹         | 59/858 [01:29<21:17,  1.60s/it]  7%|â–‹         | 60/858 [01:30<19:24,  1.46s/it]                                                  7%|â–‹         | 60/858 [01:30<19:24,  1.46s/it]  7%|â–‹         | 61/858 [01:31<17:51,  1.34s/it]  7%|â–‹         | 62/858 [01:33<21:20,  1.61s/it]  7%|â–‹         | 63/858 [01:35<22:20,  1.69s/it]  7%|â–‹         | 64/858 [01:37<22:25,  1.69s/it]  8%|â–Š         | 65/858 [01:38<20:39,  1.56s/it]  8%|â–Š         | 66/858 [01:40<19:46,  1.50s/it]  8%|â–Š         | 67/858 [01:41<18:59,  1.44s/it]  8%|â–Š         | 68/858 [01:42<18:11,  1.38s/it]  8%|â–Š         | 69/858 [01:43<17:27,  1.33s/i{'loss': 0.9828, 'grad_norm': 0.29105454683303833, 'learning_rate': 0.000134, 'epoch': 0.08}
{'loss': 0.9256, 'grad_norm': 0.27711912989616394, 'learning_rate': 0.000154, 'epoch': 0.09}
t]  8%|â–Š         | 70/858 [01:44<16:39,  1.27s/it]                                                  8%|â–Š         | 70/858 [01:44<16:39,  1.27s/it]  8%|â–Š         | 71/858 [01:47<21:13,  1.62s/it]  8%|â–Š         | 72/858 [01:48<20:53,  1.60s/it]  9%|â–Š         | 73/858 [01:50<19:36,  1.50s/it]  9%|â–Š         | 74/858 [01:51<19:51,  1.52s/it]  9%|â–Š         | 75/858 [01:52<18:05,  1.39s/it]  9%|â–‰         | 76/858 [01:54<18:09,  1.39s/it]  9%|â–‰         | 77/858 [01:56<21:59,  1.69s/it]  9%|â–‰         | 78/858 [01:57<20:29,  1.58s/it]  9%|â–‰         | 79/858 [01:58<18:28,  1.42s/it]  9%|â–‰         | 80/858 [02:00<19:28,  1.50s/it]                                                  9%|â–‰         | 80/858 [02:00<19:28,  1.50s/it]  9%|â–‰         | 81/858 [02:01<18:04,  1.40s/it] 10%|â–‰         | 82/858 [02:03<18:55,  1.46s/it] 10%|â–‰         | 83/858 [02:04<17:31,  1.36s/it] 10%|â–‰         | 84/858 [02:06<19:05,  1.48s/it] 10%|â–‰         | 85/858 [02:08<20:12,  1.57s/it] 10{'loss': 0.9007, 'grad_norm': 0.2798973619937897, 'learning_rate': 0.000174, 'epoch': 0.1}
{'loss': 0.8376, 'grad_norm': 0.2853489816188812, 'learning_rate': 0.000194, 'epoch': 0.12}
%|â–ˆ         | 86/858 [02:09<18:43,  1.45s/it] 10%|â–ˆ         | 87/858 [02:10<19:32,  1.52s/it] 10%|â–ˆ         | 88/858 [02:12<18:21,  1.43s/it] 10%|â–ˆ         | 89/858 [02:13<17:08,  1.34s/it] 10%|â–ˆ         | 90/858 [02:14<16:15,  1.27s/it]                                                 10%|â–ˆ         | 90/858 [02:14<16:15,  1.27s/it] 11%|â–ˆ         | 91/858 [02:15<16:45,  1.31s/it] 11%|â–ˆ         | 92/858 [02:16<15:43,  1.23s/it] 11%|â–ˆ         | 93/858 [02:18<16:55,  1.33s/it] 11%|â–ˆ         | 94/858 [02:19<16:24,  1.29s/it] 11%|â–ˆ         | 95/858 [02:20<15:12,  1.20s/it] 11%|â–ˆ         | 96/858 [02:21<14:51,  1.17s/it] 11%|â–ˆâ–        | 97/858 [02:22<14:32,  1.15s/it] 11%|â–ˆâ–        | 98/858 [02:25<21:43,  1.72s/it] 12%|â–ˆâ–        | 99/858 [02:27<21:51,  1.73s/it] 12%|â–ˆâ–        | 100/858 [02:28<20:14,  1.60s/it]                                                  12%|â–ˆâ–        | 100/858 [02:28<20:14,  1.60s/it] 12%|â–ˆâ–        | 101/858 [02:31<25:49,  2.0{'loss': 0.8554, 'grad_norm': 0.21440188586711884, 'learning_rate': 0.000199957917943402, 'epoch': 0.13}
5s/it] 12%|â–ˆâ–        | 102/858 [02:33<23:30,  1.87s/it] 12%|â–ˆâ–        | 103/858 [02:34<20:46,  1.65s/it] 12%|â–ˆâ–        | 104/858 [02:35<18:05,  1.44s/it] 12%|â–ˆâ–        | 105/858 [02:37<19:27,  1.55s/it] 12%|â–ˆâ–        | 106/858 [02:39<22:34,  1.80s/it] 12%|â–ˆâ–        | 107/858 [02:40<20:19,  1.62s/it] 13%|â–ˆâ–Ž        | 108/858 [02:41<18:03,  1.45s/it] 13%|â–ˆâ–Ž        | 109/858 [02:44<23:11,  1.86s/it] 13%|â–ˆâ–Ž        | 110/858 [02:47<27:41,  2.22s/it]                                                  13%|â–ˆâ–Ž        | 110/858 [02:47<27:41,  2.22s/it] 13%|â–ˆâ–Ž        | 111/858 [02:49<24:06,  1.94s/it] 13%|â–ˆâ–Ž        | 112/858 [02:50<20:53,  1.68s/it] 13%|â–ˆâ–Ž        | 113/858 [02:51<19:58,  1.61s/it] 13%|â–ˆâ–Ž        | 114/858 [02:52<18:32,  1.49s/it] 13%|â–ˆâ–Ž        | 115/858 [02:53<16:58,  1.37s/it] 14%|â–ˆâ–Ž        | 116/858 [02:54<15:42,  1.27s/it] 14%|â–ˆâ–Ž        | 117/858 [02:56<15:48,  1.28s/it] 14%|â–ˆâ–        | 118/858 [02:57<17:05,  1.39{'loss': 0.8388, 'grad_norm': 0.25630733370780945, 'learning_rate': 0.00019975188700607882, 'epoch': 0.14}
{'loss': 0.7495, 'grad_norm': 0.23955491185188293, 'learning_rate': 0.00019937453124821487, 'epoch': 0.15}
s/it] 14%|â–ˆâ–        | 119/858 [02:59<16:12,  1.32s/it] 14%|â–ˆâ–        | 120/858 [03:00<15:43,  1.28s/it]                                                  14%|â–ˆâ–        | 120/858 [03:00<15:43,  1.28s/it] 14%|â–ˆâ–        | 121/858 [03:01<15:00,  1.22s/it] 14%|â–ˆâ–        | 122/858 [03:03<19:20,  1.58s/it] 14%|â–ˆâ–        | 123/858 [03:05<18:14,  1.49s/it] 14%|â–ˆâ–        | 124/858 [03:06<17:29,  1.43s/it] 15%|â–ˆâ–        | 125/858 [03:09<23:16,  1.91s/it] 15%|â–ˆâ–        | 126/858 [03:10<21:09,  1.73s/it] 15%|â–ˆâ–        | 127/858 [03:11<18:37,  1.53s/it] 15%|â–ˆâ–        | 128/858 [03:14<22:07,  1.82s/it] 15%|â–ˆâ–Œ        | 129/858 [03:15<20:04,  1.65s/it] 15%|â–ˆâ–Œ        | 130/858 [03:16<18:16,  1.51s/it]                                                  15%|â–ˆâ–Œ        | 130/858 [03:16<18:16,  1.51s/it] 15%|â–ˆâ–Œ        | 131/858 [03:17<16:42,  1.38s/it] 15%|â–ˆâ–Œ        | 132/858 [03:19<18:05,  1.50s/it] 16%|â–ˆâ–Œ        | 133/858 [03:20<17:53,  1.48s/it{'loss': 0.776, 'grad_norm': 0.28287944197654724, 'learning_rate': 0.00019882649878193544, 'epoch': 0.16}
] 16%|â–ˆâ–Œ        | 134/858 [03:22<17:01,  1.41s/it] 16%|â–ˆâ–Œ        | 135/858 [03:23<16:26,  1.36s/it] 16%|â–ˆâ–Œ        | 136/858 [03:26<22:41,  1.89s/it] 16%|â–ˆâ–Œ        | 137/858 [03:27<20:29,  1.70s/it] 16%|â–ˆâ–Œ        | 138/858 [03:29<20:22,  1.70s/it] 16%|â–ˆâ–Œ        | 139/858 [03:30<18:00,  1.50s/it] 16%|â–ˆâ–‹        | 140/858 [03:31<16:45,  1.40s/it]                                                  16%|â–ˆâ–‹        | 140/858 [03:31<16:45,  1.40s/it] 16%|â–ˆâ–‹        | 141/858 [03:32<15:12,  1.27s/it] 17%|â–ˆâ–‹        | 142/858 [03:34<17:00,  1.42s/it] 17%|â–ˆâ–‹        | 143/858 [03:36<17:44,  1.49s/it] 17%|â–ˆâ–‹        | 144/858 [03:37<16:28,  1.38s/it] 17%|â–ˆâ–‹        | 145/858 [03:39<18:19,  1.54s/it] 17%|â–ˆâ–‹        | 146/858 [03:40<17:21,  1.46s/it] 17%|â–ˆâ–‹        | 147/858 [03:41<16:24,  1.38s/it] 17%|â–ˆâ–‹        | 148/858 [03:42<15:56,  1.35s/it] 17%|â–ˆâ–‹        | 149/858 [03:43<14:39,  1.24s/it] 17%|â–ˆâ–‹        | 150/858 [03:46<21:06,  1.79s/it]{'loss': 0.7644, 'grad_norm': 0.226559579372406, 'learning_rate': 0.00019810873085824603, 'epoch': 0.17}
{'loss': 0.7545, 'grad_norm': 0.21684600412845612, 'learning_rate': 0.00019722246025042438, 'epoch': 0.19}
                                                  17%|â–ˆâ–‹        | 150/858 [03:46<21:06,  1.79s/it] 18%|â–ˆâ–Š        | 151/858 [03:48<19:46,  1.68s/it] 18%|â–ˆâ–Š        | 152/858 [03:50<22:20,  1.90s/it] 18%|â–ˆâ–Š        | 153/858 [03:53<26:30,  2.26s/it] 18%|â–ˆâ–Š        | 154/858 [03:55<23:31,  2.00s/it] 18%|â–ˆâ–Š        | 155/858 [03:57<23:31,  2.01s/it] 18%|â–ˆâ–Š        | 156/858 [03:58<21:24,  1.83s/it] 18%|â–ˆâ–Š        | 157/858 [04:00<19:51,  1.70s/it] 18%|â–ˆâ–Š        | 158/858 [04:01<17:52,  1.53s/it] 19%|â–ˆâ–Š        | 159/858 [04:02<17:17,  1.48s/it] 19%|â–ˆâ–Š        | 160/858 [04:04<17:03,  1.47s/it]                                                  19%|â–ˆâ–Š        | 160/858 [04:04<17:03,  1.47s/it] 19%|â–ˆâ–‰        | 161/858 [04:05<15:50,  1.36s/it] 19%|â–ˆâ–‰        | 162/858 [04:06<17:09,  1.48s/it] 19%|â–ˆâ–‰        | 163/858 [04:08<16:14,  1.40s/it] 19%|â–ˆâ–‰        | 164/858 [04:10<18:04,  1.56s/it] 19%|â–ˆâ–‰        | 165/858 [04:11<17:42,  1.53s/it] 19{'loss': 0.7579, 'grad_norm': 0.2138487696647644, 'learning_rate': 0.00019616920913672093, 'epoch': 0.2}
{'loss': 0.7796, 'grad_norm': 0.2245059758424759, 'learning_rate': 0.00019495078648600287, 'epoch': 0.21}
%|â–ˆâ–‰        | 166/858 [04:12<16:25,  1.42s/it] 19%|â–ˆâ–‰        | 167/858 [04:14<16:03,  1.39s/it] 20%|â–ˆâ–‰        | 168/858 [04:15<16:43,  1.45s/it] 20%|â–ˆâ–‰        | 169/858 [04:16<15:43,  1.37s/it] 20%|â–ˆâ–‰        | 170/858 [04:18<18:11,  1.59s/it]                                                  20%|â–ˆâ–‰        | 170/858 [04:18<18:11,  1.59s/it] 20%|â–ˆâ–‰        | 171/858 [04:21<20:10,  1.76s/it] 20%|â–ˆâ–ˆ        | 172/858 [04:22<18:05,  1.58s/it] 20%|â–ˆâ–ˆ        | 173/858 [04:23<18:33,  1.63s/it] 20%|â–ˆâ–ˆ        | 174/858 [04:25<17:11,  1.51s/it] 20%|â–ˆâ–ˆ        | 175/858 [04:26<15:45,  1.38s/it] 21%|â–ˆâ–ˆ        | 176/858 [04:27<14:21,  1.26s/it] 21%|â–ˆâ–ˆ        | 177/858 [04:28<14:54,  1.31s/it] 21%|â–ˆâ–ˆ        | 178/858 [04:29<14:07,  1.25s/it] 21%|â–ˆâ–ˆ        | 179/858 [04:31<15:58,  1.41s/it] 21%|â–ˆâ–ˆ        | 180/858 [04:33<16:56,  1.50s/it]                                                  21%|â–ˆâ–ˆ        | 180/858 [04:33<16:56,  1.50s/it] 21%|â–{'loss': 0.7475, 'grad_norm': 0.23719528317451477, 'learning_rate': 0.00019356928495083297, 'epoch': 0.22}
ˆâ–ˆ        | 181/858 [04:34<15:53,  1.41s/it] 21%|â–ˆâ–ˆ        | 182/858 [04:36<16:27,  1.46s/it] 21%|â–ˆâ–ˆâ–       | 183/858 [04:37<15:33,  1.38s/it] 21%|â–ˆâ–ˆâ–       | 184/858 [04:38<15:36,  1.39s/it] 22%|â–ˆâ–ˆâ–       | 185/858 [04:39<14:44,  1.31s/it] 22%|â–ˆâ–ˆâ–       | 186/858 [04:41<16:39,  1.49s/it] 22%|â–ˆâ–ˆâ–       | 187/858 [04:43<16:26,  1.47s/it] 22%|â–ˆâ–ˆâ–       | 188/858 [04:45<18:35,  1.66s/it] 22%|â–ˆâ–ˆâ–       | 189/858 [04:46<17:01,  1.53s/it] 22%|â–ˆâ–ˆâ–       | 190/858 [04:48<17:58,  1.61s/it]                                                  22%|â–ˆâ–ˆâ–       | 190/858 [04:48<17:58,  1.61s/it] 22%|â–ˆâ–ˆâ–       | 191/858 [04:49<16:56,  1.52s/it] 22%|â–ˆâ–ˆâ–       | 192/858 [04:51<17:11,  1.55s/it] 22%|â–ˆâ–ˆâ–       | 193/858 [04:53<18:36,  1.68s/it] 23%|â–ˆâ–ˆâ–Ž       | 194/858 [04:56<23:05,  2.09s/it] 23%|â–ˆâ–ˆâ–Ž       | 195/858 [04:57<21:27,  1.94s/it] 23%|â–ˆâ–ˆâ–Ž       | 196/858 [04:59<18:56,  1.72s/it] 23%|â–ˆâ–ˆâ–Ž       | 197/858 [{'loss': 0.7104, 'grad_norm': 0.23362071812152863, 'learning_rate': 0.0001920270772733185, 'epoch': 0.23}
{'loss': 0.7367, 'grad_norm': 0.2620050609111786, 'learning_rate': 0.0001903268122099043, 'epoch': 0.24}
05:00<17:07,  1.55s/it] 23%|â–ˆâ–ˆâ–Ž       | 198/858 [05:01<17:13,  1.57s/it] 23%|â–ˆâ–ˆâ–Ž       | 199/858 [05:02<15:36,  1.42s/it] 23%|â–ˆâ–ˆâ–Ž       | 200/858 [05:03<14:13,  1.30s/it]                                                  23%|â–ˆâ–ˆâ–Ž       | 200/858 [05:03<14:13,  1.30s/it] 23%|â–ˆâ–ˆâ–Ž       | 201/858 [05:05<13:38,  1.25s/it] 24%|â–ˆâ–ˆâ–Ž       | 202/858 [05:06<13:13,  1.21s/it] 24%|â–ˆâ–ˆâ–Ž       | 203/858 [05:08<16:08,  1.48s/it] 24%|â–ˆâ–ˆâ–       | 204/858 [05:09<16:15,  1.49s/it] 24%|â–ˆâ–ˆâ–       | 205/858 [05:10<15:01,  1.38s/it] 24%|â–ˆâ–ˆâ–       | 206/858 [05:12<14:14,  1.31s/it] 24%|â–ˆâ–ˆâ–       | 207/858 [05:13<13:57,  1.29s/it] 24%|â–ˆâ–ˆâ–       | 208/858 [05:14<14:25,  1.33s/it] 24%|â–ˆâ–ˆâ–       | 209/858 [05:15<14:02,  1.30s/it] 24%|â–ˆâ–ˆâ–       | 210/858 [05:17<13:22,  1.24s/it]                                                  24%|â–ˆâ–ˆâ–       | 210/858 [05:17<13:22,  1.24s/it] 25%|â–ˆâ–ˆâ–       | 211/858 [05:18<13:06,  1.22s/it] 2{'loss': 0.7356, 'grad_norm': 0.2610858678817749, 'learning_rate': 0.00018847140998210806, 'epoch': 0.26}
5%|â–ˆâ–ˆâ–       | 212/858 [05:19<13:51,  1.29s/it] 25%|â–ˆâ–ˆâ–       | 213/858 [05:21<14:07,  1.31s/it] 25%|â–ˆâ–ˆâ–       | 214/858 [05:22<13:26,  1.25s/it] 25%|â–ˆâ–ˆâ–Œ       | 215/858 [05:23<13:42,  1.28s/it] 25%|â–ˆâ–ˆâ–Œ       | 216/858 [05:24<12:51,  1.20s/it] 25%|â–ˆâ–ˆâ–Œ       | 217/858 [05:25<12:32,  1.17s/it] 25%|â–ˆâ–ˆâ–Œ       | 218/858 [05:27<13:26,  1.26s/it] 26%|â–ˆâ–ˆâ–Œ       | 219/858 [05:28<14:46,  1.39s/it] 26%|â–ˆâ–ˆâ–Œ       | 220/858 [05:29<14:01,  1.32s/it]                                                  26%|â–ˆâ–ˆâ–Œ       | 220/858 [05:29<14:01,  1.32s/it] 26%|â–ˆâ–ˆâ–Œ       | 221/858 [05:31<14:21,  1.35s/it] 26%|â–ˆâ–ˆâ–Œ       | 222/858 [05:32<14:05,  1.33s/it] 26%|â–ˆâ–ˆâ–Œ       | 223/858 [05:34<15:03,  1.42s/it] 26%|â–ˆâ–ˆâ–Œ       | 224/858 [05:35<14:08,  1.34s/it] 26%|â–ˆâ–ˆâ–Œ       | 225/858 [05:36<13:15,  1.26s/it] 26%|â–ˆâ–ˆâ–‹       | 226/858 [05:37<13:25,  1.27s/it] 26%|â–ˆâ–ˆâ–‹       | 227/858 [05:38<12:59,  1.24s/it] 27%|â–ˆâ–ˆâ–‹       | {'loss': 0.7298, 'grad_norm': 0.19042780995368958, 'learning_rate': 0.0001864640572610118, 'epoch': 0.27}
{'loss': 0.697, 'grad_norm': 0.24045245349407196, 'learning_rate': 0.00018430820169412413, 'epoch': 0.28}
228/858 [05:39<12:20,  1.18s/it] 27%|â–ˆâ–ˆâ–‹       | 229/858 [05:41<14:51,  1.42s/it] 27%|â–ˆâ–ˆâ–‹       | 230/858 [05:43<15:37,  1.49s/it]                                                  27%|â–ˆâ–ˆâ–‹       | 230/858 [05:43<15:37,  1.49s/it] 27%|â–ˆâ–ˆâ–‹       | 231/858 [05:44<14:39,  1.40s/it] 27%|â–ˆâ–ˆâ–‹       | 232/858 [05:45<13:36,  1.30s/it] 27%|â–ˆâ–ˆâ–‹       | 233/858 [05:47<13:30,  1.30s/it] 27%|â–ˆâ–ˆâ–‹       | 234/858 [05:49<18:00,  1.73s/it] 27%|â–ˆâ–ˆâ–‹       | 235/858 [05:51<17:12,  1.66s/it] 28%|â–ˆâ–ˆâ–Š       | 236/858 [05:53<18:30,  1.78s/it] 28%|â–ˆâ–ˆâ–Š       | 237/858 [05:54<16:39,  1.61s/it] 28%|â–ˆâ–ˆâ–Š       | 238/858 [05:56<16:41,  1.62s/it] 28%|â–ˆâ–ˆâ–Š       | 239/858 [05:57<16:17,  1.58s/it] 28%|â–ˆâ–ˆâ–Š       | 240/858 [05:59<16:51,  1.64s/it]                                                  28%|â–ˆâ–ˆâ–Š       | 240/858 [05:59<16:51,  1.64s/it] 28%|â–ˆâ–ˆâ–Š       | 241/858 [06:01<16:44,  1.63s/it] 28%|â–ˆâ–ˆâ–Š       | 242/858 [06:02<15:51,  1.5{'loss': 0.6992, 'grad_norm': 0.2334718257188797, 'learning_rate': 0.0001820075459840122, 'epoch': 0.29}
4s/it] 28%|â–ˆâ–ˆâ–Š       | 243/858 [06:04<16:32,  1.61s/it] 28%|â–ˆâ–ˆâ–Š       | 244/858 [06:05<15:00,  1.47s/it] 29%|â–ˆâ–ˆâ–Š       | 245/858 [06:06<14:17,  1.40s/it] 29%|â–ˆâ–ˆâ–Š       | 246/858 [06:07<13:52,  1.36s/it] 29%|â–ˆâ–ˆâ–‰       | 247/858 [06:09<13:37,  1.34s/it] 29%|â–ˆâ–ˆâ–‰       | 248/858 [06:10<14:32,  1.43s/it] 29%|â–ˆâ–ˆâ–‰       | 249/858 [06:12<14:09,  1.39s/it] 29%|â–ˆâ–ˆâ–‰       | 250/858 [06:13<13:52,  1.37s/it]                                                  29%|â–ˆâ–ˆâ–‰       | 250/858 [06:13<13:52,  1.37s/it] 29%|â–ˆâ–ˆâ–‰       | 251/858 [06:16<17:49,  1.76s/it] 29%|â–ˆâ–ˆâ–‰       | 252/858 [06:17<16:40,  1.65s/it] 29%|â–ˆâ–ˆâ–‰       | 253/858 [06:18<15:46,  1.56s/it] 30%|â–ˆâ–ˆâ–‰       | 254/858 [06:19<14:02,  1.40s/it] 30%|â–ˆâ–ˆâ–‰       | 255/858 [06:21<13:55,  1.39s/it] 30%|â–ˆâ–ˆâ–‰       | 256/858 [06:24<19:04,  1.90s/it] 30%|â–ˆâ–ˆâ–‰       | 257/858 [06:25<17:46,  1.77s/it] 30%|â–ˆâ–ˆâ–ˆ       | 258/858 [06:27<17:22,  1.74s/it] 30%|â–ˆâ–ˆâ–ˆ{'loss': 0.7285, 'grad_norm': 0.2727329432964325, 'learning_rate': 0.00017956604152887507, 'epoch': 0.3}
{'loss': 0.7371, 'grad_norm': 0.2474241852760315, 'learning_rate': 0.00017698788163597923, 'epoch': 0.31}
       | 259/858 [06:28<16:04,  1.61s/it] 30%|â–ˆâ–ˆâ–ˆ       | 260/858 [06:29<14:35,  1.46s/it]                                                  30%|â–ˆâ–ˆâ–ˆ       | 260/858 [06:29<14:35,  1.46s/it] 30%|â–ˆâ–ˆâ–ˆ       | 261/858 [06:31<13:23,  1.35s/it] 31%|â–ˆâ–ˆâ–ˆ       | 262/858 [06:32<13:25,  1.35s/it] 31%|â–ˆâ–ˆâ–ˆ       | 263/858 [06:33<12:43,  1.28s/it] 31%|â–ˆâ–ˆâ–ˆ       | 264/858 [06:34<12:28,  1.26s/it] 31%|â–ˆâ–ˆâ–ˆ       | 265/858 [06:36<12:58,  1.31s/it] 31%|â–ˆâ–ˆâ–ˆ       | 266/858 [06:37<12:34,  1.27s/it] 31%|â–ˆâ–ˆâ–ˆ       | 267/858 [06:38<12:41,  1.29s/it] 31%|â–ˆâ–ˆâ–ˆ       | 268/858 [06:40<13:39,  1.39s/it] 31%|â–ˆâ–ˆâ–ˆâ–      | 269/858 [06:42<14:47,  1.51s/it] 31%|â–ˆâ–ˆâ–ˆâ–      | 270/858 [06:43<13:51,  1.41s/it]                                                  31%|â–ˆâ–ˆâ–ˆâ–      | 270/858 [06:43<13:51,  1.41s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 271/858 [06:44<12:37,  1.29s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 272/858 [06:47<17:58,  1.84s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 273/8{'loss': 0.7027, 'grad_norm': 0.2471962571144104, 'learning_rate': 0.0001742774943196132, 'epoch': 0.33}
58 [06:49<18:03,  1.85s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 274/858 [06:52<21:32,  2.21s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 275/858 [06:54<20:19,  2.09s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 276/858 [06:55<17:54,  1.85s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 277/858 [06:56<16:19,  1.69s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 278/858 [06:59<20:27,  2.12s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 279/858 [07:01<19:03,  1.98s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 280/858 [07:02<17:17,  1.80s/it]                                                  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 280/858 [07:02<17:17,  1.80s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 281/858 [07:04<15:46,  1.64s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 282/858 [07:05<14:08,  1.47s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 283/858 [07:06<14:01,  1.46s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 284/858 [07:07<12:38,  1.32s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 285/858 [07:08<11:43,  1.23s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 286/858 [07:10<13:20,  1.40s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 287/858 [07:11<12:58,  1.36s/it] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 288/858 [07:12<12:14,  1.29s/it] 34%|â–ˆâ–ˆâ–ˆâ–Ž{'loss': 0.7275, 'grad_norm': 0.24390718340873718, 'learning_rate': 0.0001714395346959308, 'epoch': 0.34}
{'loss': 0.7296, 'grad_norm': 0.2748525142669678, 'learning_rate': 0.00016847887698774445, 'epoch': 0.35}
      | 289/858 [07:13<11:46,  1.24s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 290/858 [07:15<12:19,  1.30s/it]                                                  34%|â–ˆâ–ˆâ–ˆâ–      | 290/858 [07:15<12:19,  1.30s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 291/858 [07:17<13:14,  1.40s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 292/858 [07:19<17:02,  1.81s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 293/858 [07:21<16:19,  1.73s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 294/858 [07:22<15:11,  1.62s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 295/858 [07:23<14:04,  1.50s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 296/858 [07:24<12:39,  1.35s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 297/858 [07:27<17:24,  1.86s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 298/858 [07:29<16:24,  1.76s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 299/858 [07:32<20:01,  2.15s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 300/858 [07:33<17:56,  1.93s/it]                                                  35%|â–ˆâ–ˆâ–ˆâ–      | 300/858 [07:33<17:56,  1.93s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 301/858 [07:35<15:50,  1.71s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 302/858 [07:36<13:59,  1.51s/it] 35%|â–ˆâ–ˆ{'loss': 0.6835, 'grad_norm': 0.22131748497486115, 'learning_rate': 0.00016540060615300096, 'epoch': 0.36}
â–ˆâ–Œ      | 303/858 [07:37<12:37,  1.37s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 304/858 [07:38<12:49,  1.39s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 305/858 [07:39<11:55,  1.29s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 306/858 [07:40<11:33,  1.26s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 307/858 [07:41<10:50,  1.18s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 308/858 [07:43<11:04,  1.21s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 309/858 [07:46<16:10,  1.77s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 310/858 [07:49<19:08,  2.10s/it]                                                  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 310/858 [07:49<19:08,  2.10s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 311/858 [07:50<17:17,  1.90s/it] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 312/858 [07:51<15:14,  1.68s/it] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 313/858 [07:53<14:32,  1.60s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 314/858 [07:55<17:17,  1.91s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 315/858 [07:57<15:53,  1.76s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 316/858 [07:58<14:10,  1.57s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 317/858 [07:59<12:32,  1.39s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 318/858 [08:00<12:28,  1.39s/it{'loss': 0.7677, 'grad_norm': 0.2655523121356964, 'learning_rate': 0.00016221000915131746, 'epoch': 0.37}
{'loss': 0.7138, 'grad_norm': 0.26295679807662964, 'learning_rate': 0.00015891256586357782, 'epoch': 0.38}
] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 319/858 [08:01<11:40,  1.30s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 320/858 [08:03<11:57,  1.33s/it]                                                  37%|â–ˆâ–ˆâ–ˆâ–‹      | 320/858 [08:03<11:57,  1.33s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 321/858 [08:04<10:59,  1.23s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 322/858 [08:05<12:04,  1.35s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 323/858 [08:07<12:00,  1.35s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 324/858 [08:08<11:18,  1.27s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 325/858 [08:09<11:29,  1.29s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 326/858 [08:10<11:00,  1.24s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 327/858 [08:13<14:44,  1.67s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 328/858 [08:14<13:42,  1.55s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 329/858 [08:15<12:37,  1.43s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 330/858 [08:17<14:26,  1.64s/it]                                                  38%|â–ˆâ–ˆâ–ˆâ–Š      | 330/858 [08:17<14:26,  1.64s/it] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 331/858 [08:19<13:22,  1.52s/it] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 332/858 [08:20<12:41,  1.{'loss': 0.6664, 'grad_norm': 0.2447386384010315, 'learning_rate': 0.0001555139396801847, 'epoch': 0.4}
45s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 333/858 [08:21<12:27,  1.42s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 334/858 [08:23<12:06,  1.39s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 335/858 [08:24<11:55,  1.37s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 336/858 [08:25<11:42,  1.35s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 337/858 [08:26<11:05,  1.28s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 338/858 [08:28<12:40,  1.46s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 339/858 [08:29<12:00,  1.39s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 340/858 [08:31<12:52,  1.49s/it]                                                  40%|â–ˆâ–ˆâ–ˆâ–‰      | 340/858 [08:31<12:52,  1.49s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 341/858 [08:32<12:19,  1.43s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 342/858 [08:34<12:17,  1.43s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 343/858 [08:36<12:52,  1.50s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 344/858 [08:37<12:21,  1.44s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 345/858 [08:39<13:25,  1.57s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 346/858 [08:41<13:53,  1.63s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 347/858 [08:42<13:29,  1.58s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 348/858 [08:{'loss': 0.6329, 'grad_norm': 0.28451141715049744, 'learning_rate': 0.0001520199677741323, 'epoch': 0.41}
{'loss': 0.7153, 'grad_norm': 0.2506590187549591, 'learning_rate': 0.00014843665107560597, 'epoch': 0.42}
43<12:54,  1.52s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 349/858 [08:45<12:30,  1.47s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 350/858 [08:46<13:03,  1.54s/it]                                                  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 350/858 [08:46<13:03,  1.54s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 351/858 [08:48<12:27,  1.47s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 352/858 [08:49<11:53,  1.41s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 353/858 [08:50<11:06,  1.32s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 354/858 [08:53<15:27,  1.84s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 355/858 [08:55<14:15,  1.70s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 356/858 [08:56<13:34,  1.62s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 357/858 [08:59<17:09,  2.05s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 358/858 [09:01<16:06,  1.93s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 359/858 [09:02<15:10,  1.82s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 360/858 [09:05<18:18,  2.21s/it]                                                  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 360/858 [09:05<18:18,  2.21s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 361/858 [09:07<16:15,  1.96s/it] 42%|â–ˆâ–ˆâ–{'loss': 0.6454, 'grad_norm': 0.2224201112985611, 'learning_rate': 0.0001447701439653271, 'epoch': 0.43}
ˆâ–ˆâ–     | 362/858 [09:08<14:03,  1.70s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 363/858 [09:10<14:17,  1.73s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 364/858 [09:11<12:57,  1.57s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 365/858 [09:12<11:35,  1.41s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 366/858 [09:13<11:15,  1.37s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 367/858 [09:14<10:16,  1.26s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 368/858 [09:15<09:55,  1.21s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 369/858 [09:16<09:17,  1.14s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 370/858 [09:17<09:15,  1.14s/it]                                                  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 370/858 [09:17<09:15,  1.14s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 371/858 [09:18<09:02,  1.11s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 372/858 [09:20<08:58,  1.11s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 373/858 [09:21<10:10,  1.26s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 374/858 [09:22<10:13,  1.27s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 375/858 [09:23<09:42,  1.21s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 376/858 [09:25<09:23,  1.17s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     |{'loss': 0.6569, 'grad_norm': 0.22565628588199615, 'learning_rate': 0.00014102674370434546, 'epoch': 0.44}
{'loss': 0.6551, 'grad_norm': 0.2818741202354431, 'learning_rate': 0.00013721287961843297, 'epoch': 0.45}
 377/858 [09:26<08:54,  1.11s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 378/858 [09:27<08:39,  1.08s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 379/858 [09:28<08:41,  1.09s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 380/858 [09:29<09:11,  1.15s/it]                                                  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 380/858 [09:29<09:11,  1.15s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 381/858 [09:30<09:26,  1.19s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 382/858 [09:32<10:01,  1.26s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 383/858 [09:33<09:43,  1.23s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 384/858 [09:34<09:39,  1.22s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 385/858 [09:35<09:13,  1.17s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 386/858 [09:36<09:33,  1.21s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 387/858 [09:38<11:05,  1.41s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 388/858 [09:41<13:28,  1.72s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 389/858 [09:42<12:29,  1.60s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 390/858 [09:43<11:14,  1.44s/it]                                                  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 390/858 [09:43<11:14,{'loss': 0.6526, 'grad_norm': 0.2222386747598648, 'learning_rate': 0.00013333510205565516, 'epoch': 0.47}
  1.44s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 391/858 [09:44<10:09,  1.30s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 392/858 [09:45<09:47,  1.26s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 393/858 [09:47<10:14,  1.32s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 394/858 [09:48<10:19,  1.34s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 395/858 [09:49<09:52,  1.28s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 396/858 [09:51<10:13,  1.33s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 397/858 [09:52<10:41,  1.39s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 398/858 [09:53<09:55,  1.29s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 399/858 [09:55<10:15,  1.34s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 400/858 [09:56<09:33,  1.25s/it]                                                  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 400/858 [09:56<09:33,  1.25s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 401/858 [09:59<13:43,  1.80s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 402/858 [10:00<12:45,  1.68s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 403/858 [10:01<11:24,  1.51s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 404/858 [10:03<11:08,  1.47s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 405/858 [10:06<14:49,  1.96s/it] {'loss': 0.6523, 'grad_norm': 0.28084808588027954, 'learning_rate': 0.0001294000711360857, 'epoch': 0.48}
47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 406/858 [10:07<13:37,  1.81s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 407/858 [10:08<11:55,  1.59s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 408/858 [10:09<10:47,  1.44s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 409/858 [10:10<09:43,  1.30s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 410/858 [10:12<09:50,  1.32s/it]                                                  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 410/858 [10:12<09:50,  1.32s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 411/858 [10:13<09:09,  1.23s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 412/858 [10:14<08:38,  1.16s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 413/858 [10:15<08:45,  1.18s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 414/858 [10:16<08:34,  1.16s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 415/858 [10:18<10:14,  1.39s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 416/858 [10:19<09:59,  1.36s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 417/858 [10:20<09:28,  1.29s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 418/858 [10:22<09:18,  1.27s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 419/858 [10:23<09:21,  1.28s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 420/858 [10:24<09:13,  1.26s/it]              {'loss': 0.6323, 'grad_norm': 0.2999787926673889, 'learning_rate': 0.0001254145453129866, 'epoch': 0.49}
{'loss': 0.6398, 'grad_norm': 0.30812323093414307, 'learning_rate': 0.00012138536976509973, 'epoch': 0.5}
                                    49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 420/858 [10:24<09:13,  1.26s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 421/858 [10:25<08:47,  1.21s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 422/858 [10:26<08:41,  1.20s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 423/858 [10:28<08:58,  1.24s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 424/858 [10:30<11:36,  1.61s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 425/858 [10:32<11:41,  1.62s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 426/858 [10:33<10:49,  1.50s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 427/858 [10:35<11:43,  1.63s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 428/858 [10:36<10:40,  1.49s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 429/858 [10:37<09:50,  1.38s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 430/858 [10:39<09:23,  1.32s/it]                                                  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 430/858 [10:39<09:23,  1.32s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 431/858 [10:40<09:02,  1.27s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 432/858 [10:41<08:43,  1.23s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 433/858 [10:43<10:31,  1.49s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 434/858 [{'loss': 0.6372, 'grad_norm': 0.2875775694847107, 'learning_rate': 0.00011731946463998711, 'epoch': 0.51}
10:46<13:58,  1.98s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 435/858 [10:48<14:18,  2.03s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 436/858 [10:51<15:04,  2.14s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 437/858 [10:52<13:12,  1.88s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 438/858 [10:55<15:37,  2.23s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 439/858 [10:56<13:50,  1.98s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 440/858 [10:57<11:50,  1.70s/it]                                                  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 440/858 [10:57<11:50,  1.70s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 441/858 [10:59<11:58,  1.72s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 442/858 [11:01<11:17,  1.63s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 443/858 [11:02<11:01,  1.60s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 444/858 [11:03<10:30,  1.52s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 445/858 [11:05<10:15,  1.49s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 446/858 [11:06<09:25,  1.37s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 447/858 [11:08<11:40,  1.70s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 448/858 [11:10<10:51,  1.59s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | {'loss': 0.6616, 'grad_norm': 0.24391162395477295, 'learning_rate': 0.00011322381316861112, 'epoch': 0.52}
{'loss': 0.6241, 'grad_norm': 0.28420954942703247, 'learning_rate': 0.00010910544967156849, 'epoch': 0.54}
449/858 [11:11<10:12,  1.50s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 450/858 [11:12<09:24,  1.38s/it]                                                  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 450/858 [11:12<09:24,  1.38s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 451/858 [11:13<08:40,  1.28s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 452/858 [11:14<08:15,  1.22s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 453/858 [11:16<08:18,  1.23s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 454/858 [11:17<08:03,  1.20s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 455/858 [11:18<08:47,  1.31s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 456/858 [11:19<08:25,  1.26s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 457/858 [11:20<08:10,  1.22s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 458/858 [11:21<07:40,  1.15s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 459/858 [11:23<07:58,  1.20s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 460/858 [11:24<07:55,  1.19s/it]                                                  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 460/858 [11:24<07:55,  1.19s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 461/858 [11:26<09:19,  1.41s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ{'loss': 0.6597, 'grad_norm': 0.290304958820343, 'learning_rate': 0.0001049714474775774, 'epoch': 0.55}
–    | 462/858 [11:28<11:13,  1.70s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 463/858 [11:30<10:30,  1.60s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 464/858 [11:31<09:50,  1.50s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 465/858 [11:32<09:47,  1.49s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 466/858 [11:35<11:20,  1.74s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 467/858 [11:36<10:29,  1.61s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 468/858 [11:37<09:13,  1.42s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 469/858 [11:38<09:24,  1.45s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 470/858 [11:41<10:43,  1.66s/it]                                                  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 470/858 [11:41<10:43,  1.66s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 471/858 [11:42<10:02,  1.56s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 472/858 [11:43<09:21,  1.45s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 473/858 [11:45<09:21,  1.46s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 474/858 [11:46<08:40,  1.36s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 475/858 [11:47<08:28,  1.33s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 476/858 [11:48<08:22,  1.32s/it] 5{'loss': 0.6678, 'grad_norm': 0.2936251759529114, 'learning_rate': 0.00010082890677496766, 'epoch': 0.56}
6%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 477/858 [11:49<08:01,  1.26s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 478/858 [11:51<09:13,  1.46s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 479/858 [11:53<10:07,  1.60s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 480/858 [11:55<09:29,  1.51s/it]                                                  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 480/858 [11:55<09:29,  1.51s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 481/858 [11:56<09:01,  1.44s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 482/858 [11:59<12:00,  1.92s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 483/858 [12:00<11:02,  1.77s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 484/858 [12:02<11:41,  1.88s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 485/858 [12:04<10:27,  1.68s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 486/858 [12:05<09:09,  1.48s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 487/858 [12:06<08:15,  1.34s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 488/858 [12:09<11:25,  1.85s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 489/858 [12:10<10:33,  1.72s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 490/858 [12:12<10:06,  1.65s/it]                                             {'loss': 0.6629, 'grad_norm': 0.26643824577331543, 'learning_rate': 9.668494241703945e-05, 'epoch': 0.57}
{'loss': 0.715, 'grad_norm': 0.24750955402851105, 'learning_rate': 9.254667170223453e-05, 'epoch': 0.58}
     57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 490/858 [12:12<10:06,  1.65s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 491/858 [12:13<09:25,  1.54s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 492/858 [12:14<08:20,  1.37s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 493/858 [12:16<08:58,  1.48s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 494/858 [12:17<08:21,  1.38s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 495/858 [12:18<07:49,  1.29s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 496/858 [12:21<11:05,  1.84s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 497/858 [12:22<10:10,  1.69s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 498/858 [12:23<08:58,  1.49s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 499/858 [12:24<08:07,  1.36s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 500/858 [12:28<11:17,  1.89s/it]                                                  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 500/858 [12:28<11:17,  1.89s/it]Saving model checkpoint to //scratch/bbvz/choprahetarth/starcoder2_script/codellama-7b-hf/checkpoint-500
/u/choprahetarth/.conda/envs/scoder_2/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
loading configuration file config.json from cache at /scratch/bbvz/choprahetarth/hub/models--meta-llama--CodeLlama-7b-hf/snapshots/b462c3c99b077d341db691ec780a33156f3c1472/config.json
Model config LlamaConfig {
  "_name_or_path": "codellama/CodeLlama-7b-hf",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 16384,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.41.2",
  "use_cache": true,
  "vocab_size": 32016
}

tokenizer config file saved in //scratch/bbvz/choprahetarth/starcoder2_script/codellama-7b-hf/checkpoint-500/tokenizer_config.json
Special tokens file saved in //scratch/bbvz/choprahetarth/starcoder2_script/codellama-7b-hf/checkpoint-500/special_tokens_map.json
/u/choprahetarth/.conda/envs/scoder_2/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6601, 'grad_norm': 0.31026986241340637, 'learning_rate': 8.842120215010803e-05, 'epoch': 0.59}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 501/858 [12:31<14:53,  2.50s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 502/858 [12:33<12:39,  2.13s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 503/858 [12:34<10:39,  1.80s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 504/858 [12:35<09:58,  1.69s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 505/858 [12:37<10:16,  1.75s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 506/858 [12:39<09:58,  1.70s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 507/858 [12:40<08:56,  1.53s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 508/858 [12:41<08:36,  1.48s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 509/858 [12:42<07:59,  1.38s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 510/858 [12:44<08:21,  1.44s/it]                                                  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 510/858 [12:44<08:21,  1.44s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 511/858 [12:45<08:18,  1.44s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 512/858 [12:48<09:59,  1.73s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 513/858 [12:49<09:47,  1.70s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 514/858 [12:51<09:00,  1.57s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 515/858 [12:5{'loss': 0.6503, 'grad_norm': 0.2570708394050598, 'learning_rate': 8.431561929409571e-05, 'epoch': 0.61}
2<07:56,  1.39s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 516/858 [12:53<07:11,  1.26s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 517/858 [12:54<07:09,  1.26s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 518/858 [12:56<08:49,  1.56s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 519/858 [12:58<08:48,  1.56s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 520/858 [12:59<08:04,  1.43s/it]                                                  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 520/858 [12:59<08:04,  1.43s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 521/858 [13:00<08:18,  1.48s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 522/858 [13:02<08:01,  1.43s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 523/858 [13:04<09:45,  1.75s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 524/858 [13:06<10:19,  1.85s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 525/858 [13:09<12:18,  2.22s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 526/858 [13:11<10:53,  1.97s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 527/858 [13:12<09:25,  1.71s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 528/858 [13:14<09:33,  1.74s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 529/858 [13:15<09:33,  1.74s/it] 62%|â–ˆâ–ˆâ–ˆâ{'loss': 0.7138, 'grad_norm': 0.28996849060058594, 'learning_rate': 8.023697451204258e-05, 'epoch': 0.62}
{'loss': 0.6021, 'grad_norm': 0.26785367727279663, 'learning_rate': 7.619227291539364e-05, 'epoch': 0.63}
–ˆâ–ˆâ–ˆâ–   | 530/858 [13:17<09:36,  1.76s/it]                                                  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 530/858 [13:17<09:36,  1.76s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 531/858 [13:20<11:15,  2.07s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 532/858 [13:21<10:11,  1.88s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 533/858 [13:22<08:55,  1.65s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 534/858 [13:24<08:14,  1.53s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 535/858 [13:25<07:38,  1.42s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 536/858 [13:27<08:55,  1.66s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 537/858 [13:29<08:50,  1.65s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 538/858 [13:32<10:52,  2.04s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 539/858 [13:33<09:53,  1.86s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 540/858 [13:35<09:21,  1.76s/it]                                                  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 540/858 [13:35<09:21,  1.76s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 541/858 [13:36<09:05,  1.72s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 542/858 [1{'loss': 0.6497, 'grad_norm': 0.28486865758895874, 'learning_rate': 7.218846131784824e-05, 'epoch': 0.64}
3:39<11:07,  2.11s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 543/858 [13:41<09:57,  1.90s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 544/858 [13:43<09:54,  1.89s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 545/858 [13:44<08:56,  1.72s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 546/858 [13:47<10:36,  2.04s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 547/858 [13:48<09:30,  1.83s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 548/858 [13:50<09:20,  1.81s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 549/858 [13:51<08:29,  1.65s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 550/858 [13:52<07:46,  1.51s/it]                                                  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 550/858 [13:52<07:46,  1.51s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 551/858 [13:53<07:17,  1.43s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 552/858 [13:55<06:42,  1.32s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 553/858 [13:56<07:26,  1.46s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 554/858 [13:58<07:01,  1.39s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 555/858 [14:00<08:18,  1.65s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 556/858 [14:03<10:17,  {'loss': 0.6386, 'grad_norm': 0.26200729608535767, 'learning_rate': 6.823241630414095e-05, 'epoch': 0.65}
2.04s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 557/858 [14:04<09:42,  1.94s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 558/858 [14:06<08:59,  1.80s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 559/858 [14:07<08:05,  1.62s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 560/858 [14:09<07:39,  1.54s/it]                                                  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 560/858 [14:09<07:39,  1.54s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 561/858 [14:10<07:27,  1.51s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 562/858 [14:11<06:43,  1.36s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 563/858 [14:12<06:12,  1.26s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 564/858 [14:14<06:41,  1.37s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 565/858 [14:15<06:17,  1.29s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 566/858 [14:16<05:56,  1.22s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 567/858 [14:18<07:01,  1.45s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 568/858 [14:19<06:33,  1.36s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 569/858 [14:21<07:19,  1.52s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 570/858 [14:22<06:55,  1.44s/it]   {'loss': 0.6835, 'grad_norm': 0.2517746388912201, 'learning_rate': 6.433093241944141e-05, 'epoch': 0.66}
{'loss': 0.6735, 'grad_norm': 0.30642107129096985, 'learning_rate': 6.049071049965811e-05, 'epoch': 0.68}
                                               66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 570/858 [14:22<06:55,  1.44s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 571/858 [14:23<06:21,  1.33s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 572/858 [14:24<06:05,  1.28s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 573/858 [14:26<06:46,  1.43s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 574/858 [14:27<06:24,  1.35s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 575/858 [14:29<06:47,  1.44s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 576/858 [14:31<07:17,  1.55s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 577/858 [14:32<06:47,  1.45s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 578/858 [14:34<06:58,  1.50s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 579/858 [14:35<06:26,  1.38s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 580/858 [14:36<05:55,  1.28s/it]                                                  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 580/858 [14:36<05:55,  1.28s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 581/858 [14:39<08:22,  1.81s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 582/858 [14:40<07:49,  1.70s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   {'loss': 0.6668, 'grad_norm': 0.32166025042533875, 'learning_rate': 5.671834616268861e-05, 'epoch': 0.69}
| 583/858 [14:41<07:06,  1.55s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 584/858 [14:44<07:54,  1.73s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 585/858 [14:45<07:09,  1.57s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 586/858 [14:46<06:44,  1.49s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 587/858 [14:47<06:19,  1.40s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 588/858 [14:48<05:54,  1.31s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 589/858 [14:50<05:45,  1.28s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 590/858 [14:51<05:41,  1.27s/it]                                                  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 590/858 [14:51<05:41,  1.27s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 591/858 [14:52<05:39,  1.27s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 592/858 [14:55<07:36,  1.72s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 593/858 [14:56<07:26,  1.68s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 594/858 [14:58<06:48,  1.55s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 595/858 [14:59<06:23,  1.46s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 596/858 [15:00<06:04,  1.39s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 597/858 [15{'loss': 0.6151, 'grad_norm': 0.35348203778266907, 'learning_rate': 5.3020318480382404e-05, 'epoch': 0.7}
{'loss': 0.6173, 'grad_norm': 0.3256791830062866, 'learning_rate': 4.940297885067333e-05, 'epoch': 0.71}
:02<06:02,  1.39s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 598/858 [15:03<05:46,  1.33s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 599/858 [15:04<05:44,  1.33s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 600/858 [15:05<05:24,  1.26s/it]                                                  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 600/858 [15:05<05:24,  1.26s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 601/858 [15:07<06:25,  1.50s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 602/858 [15:09<06:13,  1.46s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 603/858 [15:10<06:27,  1.52s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 604/858 [15:11<06:00,  1.42s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 605/858 [15:12<05:30,  1.31s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 606/858 [15:14<05:12,  1.24s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 607/858 [15:15<06:05,  1.46s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 608/858 [15:17<06:00,  1.44s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 609/858 [15:18<05:47,  1.39s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 610/858 [15:19<05:30,  1.33s/it]                                                  71%{'loss': 0.649, 'grad_norm': 0.3039312958717346, 'learning_rate': 4.587254008899278e-05, 'epoch': 0.72}
|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 610/858 [15:19<05:30,  1.33s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 611/858 [15:22<07:40,  1.86s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 612/858 [15:24<07:07,  1.74s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 613/858 [15:25<06:30,  1.59s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 614/858 [15:27<06:40,  1.64s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 615/858 [15:28<06:04,  1.50s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 616/858 [15:29<05:41,  1.41s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 617/858 [15:30<05:17,  1.32s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 618/858 [15:31<05:00,  1.25s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 619/858 [15:33<04:43,  1.19s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 620/858 [15:34<04:52,  1.23s/it]                                                  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 620/858 [15:34<04:52,  1.23s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 621/858 [15:35<04:41,  1.19s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 622/858 [15:36<04:32,  1.15s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 623/858 [15:38<04:56,  1{'loss': 0.617, 'grad_norm': 0.2705158591270447, 'learning_rate': 4.2435065757699575e-05, 'epoch': 0.73}
.26s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 624/858 [15:41<06:56,  1.78s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 625/858 [15:43<07:16,  1.87s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 626/858 [15:44<06:36,  1.71s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 627/858 [15:45<05:49,  1.51s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 628/858 [15:46<05:28,  1.43s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 629/858 [15:48<05:16,  1.38s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 630/858 [15:51<07:10,  1.89s/it]                                                  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 630/858 [15:51<07:10,  1.89s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 631/858 [15:52<06:30,  1.72s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 632/858 [15:53<05:51,  1.55s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 633/858 [15:55<06:46,  1.81s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 634/858 [15:57<06:07,  1.64s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 635/858 [15:58<05:47,  1.56s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 636/858 [16:01<07:24,  2.00s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 637/858{'loss': 0.6292, 'grad_norm': 0.29319435358047485, 'learning_rate': 3.90964597518537e-05, 'epoch': 0.75}
 [16:04<08:33,  2.32s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 638/858 [16:07<08:36,  2.35s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 639/858 [16:08<07:41,  2.11s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 640/858 [16:09<06:49,  1.88s/it]                                                  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 640/858 [16:09<06:49,  1.88s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 641/858 [16:12<07:01,  1.94s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 642/858 [16:13<06:12,  1.73s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 643/858 [16:14<05:23,  1.51s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 644/858 [16:16<05:40,  1.59s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 645/858 [16:17<05:13,  1.47s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 646/858 [16:18<05:00,  1.42s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 647/858 [16:19<04:32,  1.29s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 648/858 [16:20<04:19,  1.24s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 649/858 [16:21<04:01,  1.15s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 650/858 [16:24<06:02,  1.74s/it]                       {'loss': 0.6516, 'grad_norm': 0.2289523035287857, 'learning_rate': 3.5862456159220114e-05, 'epoch': 0.76}
{'loss': 0.6421, 'grad_norm': 0.2940738797187805, 'learning_rate': 3.2738609411918e-05, 'epoch': 0.77}
                           76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 650/858 [16:24<06:02,  1.74s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 651/858 [16:26<05:54,  1.71s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 652/858 [16:27<05:26,  1.58s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 653/858 [16:29<05:24,  1.58s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 654/858 [16:30<05:29,  1.61s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 655/858 [16:32<04:55,  1.46s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 656/858 [16:33<04:29,  1.34s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 657/858 [16:34<04:17,  1.28s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 658/858 [16:35<03:57,  1.19s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 659/858 [16:38<05:48,  1.75s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 660/858 [16:39<05:29,  1.66s/it]                                                  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 660/858 [16:39<05:29,  1.66s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 661/858 [16:40<05:03,  1.54s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 662/858 [16:42<04:52,  1.49s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ{'loss': 0.6077, 'grad_norm': 0.3137885332107544, 'learning_rate': 2.9730284746630454e-05, 'epoch': 0.78}
–ˆâ–ˆâ–‹  | 663/858 [16:43<04:25,  1.36s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 664/858 [16:45<04:36,  1.42s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 665/858 [16:46<04:25,  1.37s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 666/858 [16:47<04:15,  1.33s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 667/858 [16:48<04:21,  1.37s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 668/858 [16:50<04:23,  1.39s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 669/858 [16:51<04:23,  1.39s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 670/858 [16:52<04:04,  1.30s/it]                                                  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 670/858 [16:52<04:04,  1.30s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 671/858 [16:53<03:50,  1.23s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 672/858 [16:55<04:10,  1.35s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 673/858 [16:56<04:04,  1.32s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 674/858 [16:58<04:35,  1.50s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 675/858 [16:59<04:20,  1.42s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 676/858 [17:01<04:25,  1.46s/it] 79%{'loss': 0.6385, 'grad_norm': 0.2748236060142517, 'learning_rate': 2.684264898975932e-05, 'epoch': 0.79}
|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 677/858 [17:03<04:43,  1.56s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 678/858 [17:04<04:25,  1.47s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 679/858 [17:05<04:11,  1.40s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 680/858 [17:07<04:18,  1.45s/it]                                                  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 680/858 [17:07<04:18,  1.45s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 681/858 [17:09<04:28,  1.51s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 682/858 [17:10<04:13,  1.44s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 683/858 [17:11<04:05,  1.40s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 684/858 [17:12<03:45,  1.29s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 685/858 [17:13<03:26,  1.19s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 686/858 [17:15<04:23,  1.53s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 687/858 [17:17<04:09,  1.46s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 688/858 [17:18<04:01,  1.42s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 689/858 [17:19<03:43,  1.32s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 690/858 [17:20<03:27{'loss': 0.6211, 'grad_norm': 0.30275958776474, 'learning_rate': 2.4080661683351147e-05, 'epoch': 0.8}
{'loss': 0.6139, 'grad_norm': 0.28200268745422363, 'learning_rate': 2.1449066567036413e-05, 'epoch': 0.82}
,  1.23s/it]                                                  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 690/858 [17:20<03:27,  1.23s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 691/858 [17:22<03:59,  1.43s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 692/858 [17:23<03:48,  1.38s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 693/858 [17:24<03:34,  1.30s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 694/858 [17:27<04:57,  1.82s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 695/858 [17:29<04:39,  1.71s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 696/858 [17:30<04:14,  1.57s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 697/858 [17:33<05:02,  1.88s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 698/858 [17:34<04:28,  1.68s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 699/858 [17:35<03:54,  1.47s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 700/858 [17:37<04:13,  1.60s/it]                                                  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 700/858 [17:37<04:13,  1.60s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 701/858 [17:39<04:47,  1.83s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 702/85{'loss': 0.5859, 'grad_norm': 0.2801503539085388, 'learning_rate': 1.8952383430611298e-05, 'epoch': 0.83}
8 [17:41<04:36,  1.77s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 703/858 [17:42<04:10,  1.62s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 704/858 [17:43<03:45,  1.46s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 705/858 [17:46<04:54,  1.93s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 706/858 [17:48<04:44,  1.87s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 707/858 [17:50<04:37,  1.84s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 708/858 [17:51<04:11,  1.67s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 709/858 [17:54<05:13,  2.10s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 710/858 [17:56<04:40,  1.90s/it]                                                  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 710/858 [17:56<04:40,  1.90s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 711/858 [17:57<04:10,  1.71s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 712/858 [17:58<03:51,  1.59s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 713/858 [17:59<03:26,  1.42s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 714/858 [18:01<03:47,  1.58s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 715/858 [18:02<03:34,  1.50{'loss': 0.6103, 'grad_norm': 0.3367868661880493, 'learning_rate': 1.6594900351255428e-05, 'epoch': 0.84}
s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 716/858 [18:04<03:14,  1.37s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 717/858 [18:05<03:15,  1.39s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 718/858 [18:06<03:05,  1.33s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 719/858 [18:07<02:54,  1.25s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 720/858 [18:10<03:36,  1.57s/it]                                                  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 720/858 [18:10<03:36,  1.57s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 721/858 [18:11<03:18,  1.45s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 722/858 [18:14<04:26,  1.96s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 723/858 [18:15<04:01,  1.79s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 724/858 [18:16<03:28,  1.56s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 725/858 [18:18<03:49,  1.73s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 726/858 [18:20<03:48,  1.73s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 727/858 [18:21<03:27,  1.58s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 728/858 [18:23<03:14,  1.50s/it] 85%|â–ˆâ–ˆâ–ˆâ{'loss': 0.6149, 'grad_norm': 0.29912281036376953, 'learning_rate': 1.4380666328718274e-05, 'epoch': 0.85}
{'loss': 0.6409, 'grad_norm': 0.2611428201198578, 'learning_rate': 1.2313484331123371e-05, 'epoch': 0.86}
–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 729/858 [18:24<02:58,  1.39s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 730/858 [18:25<02:42,  1.27s/it]                                                  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 730/858 [18:25<02:42,  1.27s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 731/858 [18:27<03:06,  1.47s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 732/858 [18:28<02:57,  1.41s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 733/858 [18:29<02:54,  1.39s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 734/858 [18:30<02:41,  1.30s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 735/858 [18:32<02:32,  1.24s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 736/858 [18:33<02:28,  1.21s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 737/858 [18:34<02:18,  1.14s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 738/858 [18:35<02:14,  1.12s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 739/858 [18:38<03:22,  1.70s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 740/858 [18:39<03:12,  1.63s/it]                                                  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 740/858 [18:39<03:{'loss': 0.6375, 'grad_norm': 0.2563123106956482, 'learning_rate': 1.039690476333418e-05, 'epoch': 0.87}
12,  1.63s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 741/858 [18:41<02:59,  1.54s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 742/858 [18:42<02:49,  1.46s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 743/858 [18:43<02:42,  1.41s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 744/858 [18:44<02:32,  1.34s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 745/858 [18:45<02:22,  1.26s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 746/858 [18:46<02:13,  1.19s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 747/858 [18:48<02:17,  1.23s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 748/858 [18:49<02:16,  1.24s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 749/858 [18:50<02:15,  1.24s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 750/858 [18:51<02:13,  1.24s/it]                                                  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 750/858 [18:51<02:13,  1.24s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 751/858 [18:53<02:08,  1.20s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 752/858 [18:54<02:09,  1.22s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 753/858 [18:55<02:03,  1.17s/it] 88%|â{'loss': 0.6284, 'grad_norm': 0.35397329926490784, 'learning_rate': 8.634219369099694e-06, 'epoch': 0.89}
–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 754/858 [18:56<02:01,  1.17s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 755/858 [18:57<02:07,  1.24s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 756/858 [19:01<03:01,  1.78s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 757/858 [19:02<02:47,  1.66s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 758/858 [19:03<02:40,  1.60s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 759/858 [19:05<02:31,  1.53s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 760/858 [19:06<02:21,  1.45s/it]                                                  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 760/858 [19:06<02:21,  1.45s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 761/858 [19:07<02:10,  1.35s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 762/858 [19:10<03:00,  1.88s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 763/858 [19:12<02:44,  1.73s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 764/858 [19:13<02:31,  1.61s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 765/858 [19:14<02:15,  1.46s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 766/858 [19:17<02:58,  1.94s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ{'loss': 0.6345, 'grad_norm': 0.32207316160202026, 'learning_rate': 7.028455577453074e-06, 'epoch': 0.9}
–ˆâ–‰ | 767/858 [19:19<02:51,  1.88s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 768/858 [19:20<02:30,  1.67s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 769/858 [19:21<02:21,  1.59s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 770/858 [19:23<02:11,  1.49s/it]                                                  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 770/858 [19:23<02:11,  1.49s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 771/858 [19:25<02:24,  1.66s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 772/858 [19:26<02:10,  1.52s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 773/858 [19:28<02:28,  1.75s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 774/858 [19:30<02:39,  1.90s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 775/858 [19:32<02:30,  1.82s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 776/858 [19:34<02:21,  1.72s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 777/858 [19:35<02:12,  1.63s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 778/858 [19:38<02:45,  2.07s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 779/858 [19:40<02:31,  1.92s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 780/858 [19:4{'loss': 0.6419, 'grad_norm': 0.3087430000305176, 'learning_rate': 5.582371303073386e-06, 'epoch': 0.91}
{'loss': 0.6098, 'grad_norm': 0.2966780364513397, 'learning_rate': 4.298450209540628e-06, 'epoch': 0.92}
1<02:13,  1.71s/it]                                                  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 780/858 [19:41<02:13,  1.71s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 781/858 [19:42<01:54,  1.48s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 782/858 [19:43<01:48,  1.43s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 783/858 [19:44<01:44,  1.40s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 784/858 [19:45<01:34,  1.28s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 785/858 [19:48<02:05,  1.72s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 786/858 [19:50<01:57,  1.64s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 787/858 [19:53<02:25,  2.04s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 788/858 [19:54<02:10,  1.86s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 789/858 [19:55<01:50,  1.60s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 790/858 [19:56<01:39,  1.46s/it]                                                  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 790/858 [19:56<01:39,  1.46s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 791/858 [19:58<01:36,  1.45s/it{'loss': 0.6156, 'grad_norm': 0.2919371724128723, 'learning_rate': 3.1788974436198328e-06, 'epoch': 0.93}
] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 792/858 [19:59<01:28,  1.34s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 793/858 [20:00<01:23,  1.29s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 794/858 [20:01<01:19,  1.24s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 795/858 [20:02<01:16,  1.21s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 796/858 [20:03<01:12,  1.17s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 797/858 [20:05<01:16,  1.25s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 798/858 [20:06<01:12,  1.21s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 799/858 [20:07<01:13,  1.25s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 800/858 [20:08<01:11,  1.23s/it]                                                  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 800/858 [20:08<01:11,  1.23s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 801/858 [20:09<01:08,  1.20s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 802/858 [20:11<01:07,  1.21s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 803/858 [20:12<01:12,  1.32s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 804/858 [20:14<01:12,  1{'loss': 0.6298, 'grad_norm': 0.31158241629600525, 'learning_rate': 2.225635847900409e-06, 'epoch': 0.94}
.34s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 805/858 [20:15<01:07,  1.28s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 806/858 [20:16<01:06,  1.27s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 807/858 [20:17<01:01,  1.20s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 808/858 [20:19<01:06,  1.32s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 809/858 [20:20<01:04,  1.31s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 810/858 [20:21<01:02,  1.30s/it]                                                  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 810/858 [20:21<01:02,  1.30s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 811/858 [20:23<01:04,  1.37s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 812/858 [20:24<00:59,  1.29s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 813/858 [20:25<00:58,  1.31s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 814/858 [20:27<00:59,  1.36s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 815/858 [20:28<00:55,  1.29s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 816/858 [20:30<01:00,  1.44s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 817/858 [20:32<01{'loss': 0.6128, 'grad_norm': 0.3101762533187866, 'learning_rate': 1.4403026582955337e-06, 'epoch': 0.96}
:10,  1.73s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 818/858 [20:33<01:03,  1.59s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 819/858 [20:35<00:59,  1.51s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 820/858 [20:36<00:52,  1.39s/it]                                                  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 820/858 [20:36<00:52,  1.39s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 821/858 [20:37<00:47,  1.30s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 822/858 [20:38<00:45,  1.28s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 823/858 [20:40<00:50,  1.45s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 824/858 [20:42<00:54,  1.60s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 825/858 [20:43<00:49,  1.49s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 826/858 [20:44<00:45,  1.42s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 827/858 [20:45<00:40,  1.30s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 828/858 [20:46<00:37,  1.25s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 829/858 [20:48<00:36,  1.27s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 830/858 [2{'loss': 0.5972, 'grad_norm': 0.3261730670928955, 'learning_rate': 8.242466920738601e-07, 'epoch': 0.97}
{'loss': 0.6735, 'grad_norm': 0.31434470415115356, 'learning_rate': 3.7852603125291265e-07, 'epoch': 0.98}
0:50<00:40,  1.43s/it]                                                  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 830/858 [20:50<00:40,  1.43s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 831/858 [20:51<00:36,  1.34s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 832/858 [20:52<00:33,  1.27s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 833/858 [20:53<00:29,  1.18s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 834/858 [20:54<00:28,  1.19s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 835/858 [20:55<00:26,  1.16s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 836/858 [20:57<00:28,  1.28s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 837/858 [20:58<00:27,  1.33s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 838/858 [20:59<00:25,  1.26s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 839/858 [21:01<00:24,  1.30s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 840/858 [21:02<00:24,  1.34s/it]                                                  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 840/858 [21:02<00:24,  1.34s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 841/858 [21:03<00:21, {'loss': 0.5765, 'grad_norm': 0.25390711426734924, 'learning_rate': 1.0390620533312634e-07, 'epoch': 0.99}
 1.29s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 842/858 [21:04<00:20,  1.29s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 843/858 [21:06<00:18,  1.24s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 844/858 [21:07<00:16,  1.17s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 845/858 [21:08<00:14,  1.12s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 846/858 [21:09<00:13,  1.11s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 847/858 [21:11<00:14,  1.36s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 848/858 [21:13<00:15,  1.60s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 849/858 [21:15<00:15,  1.77s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 850/858 [21:17<00:13,  1.72s/it]                                                  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 850/858 [21:17<00:13,  1.72s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 851/858 [21:18<00:11,  1.60s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 852/858 [21:19<00:08,  1.44s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 853/858 [21:20<00:06,  1.34s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 854/858 [21:21<00:04,  1.22s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 855/858 [21:22<00:03,  1.27s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 856/858 [21:24<00:02,  1.22s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 857/858 [21:26<00:01,  1.49s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 858/858 [21:29<00:00,  1.97s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


{'train_runtime': 1303.2661, 'train_samples_per_second': 10.534, 'train_steps_per_second': 0.658, 'train_loss': 0.7755802578025764, 'epoch': 1.0}
                                                 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 858/858 [21:29<00:00,  1.97s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 858/858 [21:29<00:00,  1.50s/it]
LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32016, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaSdpaAttention(
          (q_proj): lora.Linear8bitLt(
            (base_layer): Linear8bitLt(in_features=4096, out_features=4096, bias=False)
            (lora_dropout): ModuleDict(
              (default): Identity()
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=4096, out_features=16, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=16, out_features=4096, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
          )
          (k_proj): lora.Linear8bitLt(
            (base_layer): Linear8bitLt(in_features=4096, out_features=4096, bias=False)
            (lora_dropout): ModuleDict(
              (default): Identity()
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=4096, out_features=16, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=16, out_features=4096, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
          )
          (v_proj): lora.Linear8bitLt(
            (base_layer): Linear8bitLt(in_features=4096, out_features=4096, bias=False)
            (lora_dropout): ModuleDict(
              (default): Identity()
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=4096, out_features=16, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=16, out_features=4096, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
          )
          (o_proj): lora.Linear8bitLt(
            (base_layer): Linear8bitLt(in_features=4096, out_features=4096, bias=False)
            (lora_dropout): ModuleDict(
              (default): Identity()
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=4096, out_features=16, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=16, out_features=4096, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
          )
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): lora.Linear8bitLt(
            (base_layer): Linear8bitLt(in_features=4096, out_features=11008, bias=False)
            (lora_dropout): ModuleDict(
              (default): Identity()
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=4096, out_features=16, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=16, out_features=11008, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
          )
          (up_proj): lora.Linear8bitLt(
            (base_layer): Linear8bitLt(in_features=4096, out_features=11008, bias=False)
            (lora_dropout): ModuleDict(
              (default): Identity()
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=4096, out_features=16, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=16, out_features=11008, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
          )
          (down_proj): lora.Linear8bitLt(
            (base_layer): Linear8bitLt(in_features=11008, out_features=4096, bias=False)
            (lora_dropout): ModuleDict(
              (default): Identity()
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=11008, out_features=16, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=16, out_features=4096, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
          )
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32016, bias=False)
)
Saving the last checkpoint of the model
LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32016, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaSdpaAttention(
          (q_proj): lora.Linear8bitLt(
            (base_layer): Linear8bitLt(in_features=4096, out_features=4096, bias=False)
            (lora_dropout): ModuleDict(
              (default): Identity()
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=4096, out_features=16, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=16, out_features=4096, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
          )
          (k_proj): lora.Linear8bitLt(
            (base_layer): Linear8bitLt(in_features=4096, out_features=4096, bias=False)
            (lora_dropout): ModuleDict(
              (default): Identity()
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=4096, out_features=16, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=16, out_features=4096, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
          )
          (v_proj): lora.Linear8bitLt(
            (base_layer): Linear8bitLt(in_features=4096, out_features=4096, bias=False)
            (lora_dropout): ModuleDict(
              (default): Identity()
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=4096, out_features=16, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=16, out_features=4096, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
          )
          (o_proj): lora.Linear8bitLt(
            (base_layer): Linear8bitLt(in_features=4096, out_features=4096, bias=False)
            (lora_dropout): ModuleDict(
              (default): Identity()
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=4096, out_features=16, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=16, out_features=4096, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
          )
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): lora.Linear8bitLt(
            (base_layer): Linear8bitLt(in_features=4096, out_features=11008, bias=False)
            (lora_dropout): ModuleDict(
              (default): Identity()
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=4096, out_features=16, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=16, out_features=11008, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
          )
          (up_proj): lora.Linear8bitLt(
            (base_layer): Linear8bitLt(in_features=4096, out_features=11008, bias=False)
            (lora_dropout): ModuleDict(
              (default): Identity()
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=4096, out_features=16, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=16, out_features=11008, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
          )
          (down_proj): lora.Linear8bitLt(
            (base_layer): Linear8bitLt(in_features=11008, out_features=4096, bias=False)
            (lora_dropout): ModuleDict(
              (default): Identity()
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=11008, out_features=16, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=16, out_features=4096, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
          )
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32016, bias=False)
)
LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32016, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaSdpaAttention(
          (q_proj): lora.Linear8bitLt(
            (base_layer): Linear8bitLt(in_features=4096, out_features=4096, bias=False)
            (lora_dropout): ModuleDict(
              (default): Identity()
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=4096, out_features=16, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=16, out_features=4096, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
          )
          (k_proj): lora.Linear8bitLt(
            (base_layer): Linear8bitLt(in_features=4096, out_features=4096, bias=False)
            (lora_dropout): ModuleDict(
              (default): Identity()
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=4096, out_features=16, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=16, out_features=4096, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
          )
          (v_proj): lora.Linear8bitLt(
            (base_layer): Linear8bitLt(in_features=4096, out_features=4096, bias=False)
            (lora_dropout): ModuleDict(
              (default): Identity()
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=4096, out_features=16, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=16, out_features=4096, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
          )
          (o_proj): lora.Linear8bitLt(
            (base_layer): Linear8bitLt(in_features=4096, out_features=4096, bias=False)
            (lora_dropout): ModuleDict(
              (default): Identity()
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=4096, out_features=16, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=16, out_features=4096, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
          )
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): lora.Linear8bitLt(
            (base_layer): Linear8bitLt(in_features=4096, out_features=11008, bias=False)
            (lora_dropout): ModuleDict(
              (default): Identity()
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=4096, out_features=16, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=16, out_features=11008, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
          )
          (up_proj): lora.Linear8bitLt(
            (base_layer): Linear8bitLt(in_features=4096, out_features=11008, bias=False)
            (lora_dropout): ModuleDict(
              (default): Identity()
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=4096, out_features=16, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=16, out_features=11008, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
          )
          (down_proj): lora.Linear8bitLt(
            (base_layer): Linear8bitLt(in_features=11008, out_features=4096, bias=False)
            (lora_dropout): ModuleDict(
              (default): Identity()
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=11008, out_features=16, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=16, out_features=4096, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
          )
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32016, bias=False)
)
LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32016, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaSdpaAttention(
          (q_proj): lora.Linear8bitLt(
            (base_layer): Linear8bitLt(in_features=4096, out_features=4096, bias=False)
            (lora_dropout): ModuleDict(
              (default): Identity()
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=4096, out_features=16, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=16, out_features=4096, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
          )
          (k_proj): lora.Linear8bitLt(
            (base_layer): Linear8bitLt(in_features=4096, out_features=4096, bias=False)
            (lora_dropout): ModuleDict(
              (default): Identity()
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=4096, out_features=16, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=16, out_features=4096, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
          )
          (v_proj): lora.Linear8bitLt(
            (base_layer): Linear8bitLt(in_features=4096, out_features=4096, bias=False)
            (lora_dropout): ModuleDict(
              (default): Identity()
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=4096, out_features=16, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=16, out_features=4096, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
          )
          (o_proj): lora.Linear8bitLt(
            (base_layer): Linear8bitLt(in_features=4096, out_features=4096, bias=False)
            (lora_dropout): ModuleDict(
              (default): Identity()
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=4096, out_features=16, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=16, out_features=4096, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
          )
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): lora.Linear8bitLt(
            (base_layer): Linear8bitLt(in_features=4096, out_features=11008, bias=False)
            (lora_dropout): ModuleDict(
              (default): Identity()
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=4096, out_features=16, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=16, out_features=11008, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
          )
          (up_proj): lora.Linear8bitLt(
            (base_layer): Linear8bitLt(in_features=4096, out_features=11008, bias=False)
            (lora_dropout): ModuleDict(
              (default): Identity()
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=4096, out_features=16, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=16, out_features=11008, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
          )
          (down_proj): lora.Linear8bitLt(
            (base_layer): Linear8bitLt(in_features=11008, out_features=4096, bias=False)
            (lora_dropout): ModuleDict(
              (default): Identity()
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=11008, out_features=16, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=16, out_features=4096, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
          )
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32016, bias=False)
)Saving the last checkpoint of the model

Saving the last checkpoint of the model
Saving the last checkpoint of the model
Training Done! ðŸ’¥Training Done! ðŸ’¥

Training Done! ðŸ’¥
Training Done! ðŸ’¥
Configuration saved in //scratch/bbvz/choprahetarth/starcoder2_script/codellama-7b-hf/final_checkpoint/config.json
Configuration saved in //scratch/bbvz/choprahetarth/starcoder2_script/codellama-7b-hf/final_checkpoint/generation_config.json
The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at //scratch/bbvz/choprahetarth/starcoder2_script/codellama-7b-hf/final_checkpoint/model.safetensors.index.json.
wandb: - 0.008 MB of 0.008 MB uploadedwandb: \ 0.038 MB of 0.038 MB uploadedwandb: 
wandb: Run history:
wandb:         train/epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   train/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:     train/grad_norm â–ƒâ–ˆâ–â–‚â–‚â–â–â–â–â–â–‚â–â–‚â–â–â–‚â–‚â–â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–ƒâ–ƒâ–‚â–â–‚â–‚â–ƒâ–‚â–ƒâ–‚â–‚â–‚â–‚
wandb: train/learning_rate â–‚â–ƒâ–„â–†â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–…â–…â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–
wandb:          train/loss â–ˆâ–‡â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:               total_flos 7.55303663492137e+16
wandb:              train/epoch 1.0
wandb:        train/global_step 858
wandb:          train/grad_norm 0.25391
wandb:      train/learning_rate 0.0
wandb:               train/loss 0.5765
wandb:               train_loss 0.77558
wandb:            train_runtime 1303.2661
wandb: train_samples_per_second 10.534
wandb:   train_steps_per_second 0.658
wandb: 
wandb: ðŸš€ View run train-CodeLlama-7b-hf at: https://wandb.ai/hetarthvader/huggingface/runs/rnljyv1a
wandb: â­ï¸ View project at: https://wandb.ai/hetarthvader/huggingface
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/bbvz/choprahetarth/wandb/run-20240622_182610-rnljyv1a/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
